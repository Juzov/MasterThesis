@article{ROUSSEEUW198753,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an â€˜appropriate' number of clusters.},
author = {Rousseeuw, Peter J},
doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
issn = {0377-0427},
journal = {Journal of Computational and Applied Mathematics},
keywords = { classification, cluster analysis, clustering validity,Graphical display},
pages = {53--65},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
url = {http://www.sciencedirect.com/science/article/pii/0377042787901257},
volume = {20},
year = {1987}
}
@article{Halkidi2002,
abstract = {Clustering is an unsupervised process since there are no predefined classes and no examples that would indicate grouping properties in the data set. The majority of the clustering algorithms behave differently depending on the features of the data set and the initial assumptions for defining groups. Therefore, in most applications the resulting clustering scheme requires some sort of evaluation as regards its validity. Evaluating and assessing the results of a clustering algorithm is the main subject of cluster validity. In this paper we present a review of the clustering validity and methods. More specifically, Part I of the paper discusses the cluster validity approaches based on external and internal criteria.},
author = {Halkidi, Maria and Batistakis, Yannis and Vazirgiannis, Michalis},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/cval.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
number = {2},
pages = {40--45},
title = {{Cluster validity methods: part I}},
url = {http://portal.acm.org/citation.cfm?id=565117.565124},
volume = {31},
year = {2002}
}
@article{manning2010introduction,
author = {Manning, Christopher and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
journal = {Natural Language Engineering},
number = {1},
pages = {100--103},
publisher = {Cambridge university press},
title = {{Introduction to information retrieval}},
volume = {16},
year = {2010}
}
@article{DENG201684,
abstract = {Subspace clustering (SC) is a promising technology involving clusters that are identified based on their association with subspaces in high-dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been studied extensively and are well accepted by the scientific community, SSC algorithms are relatively new. However, as they are said to be more adaptable than their HSC counterparts, SSC algorithms have been attracting more attention in recent years. A comprehensive survey of existing SSC algorithms and recent developments in the field are presented in this paper. SSC algorithms have been systematically classified into three main categories: conventional SSC (CSSC), independent SSC (ISSC), and extended SSC (XSSC). The characteristics of these algorithms are highlighted and potential future developments in the area of SSC are discussed. Through a comprehensive review of SSC, this paper aims to provide readers with a clear profile of existing SSC methods and to foster the development of more effective clustering technologies and significant research in this area.},
author = {Deng, Zhaohong and Choi, Kup-Sze and Jiang, Yizhang and Wang, Jun and Wang, Shitong},
doi = {https://doi.org/10.1016/j.ins.2016.01.101},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/survey{\_}subspace.pdf:pdf},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {Entropy weighting,Fuzzy C-means/-means model,Fuzzy weighting,Mixture model,Soft subspace clustering},
pages = {84--106},
title = {{A survey on soft subspace clustering}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025516300640},
volume = {348},
year = {2016}
}
@article{Parsons:2004:SCH:1007730.1007731,
address = {New York, NY, USA},
author = {Parsons, Lance and Haque, Ehtesham and Liu, Huan},
doi = {10.1145/1007730.1007731},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/reviewsubspace.pdf:pdf},
issn = {1931-0145},
journal = {SIGKDD Explor. Newsl.},
keywords = {clustering survey,high dimensional data,projected clustering,subspace clustering},
month = {jun},
number = {1},
pages = {90--105},
publisher = {ACM},
title = {{Subspace Clustering for High Dimensional Data: A Review}},
url = {http://doi.acm.org/10.1145/1007730.1007731},
volume = {6},
year = {2004}
}
@article{Jia2018,
abstract = {In clustering analysis, data attributes may have different contributions to the detection of various clusters. To solve this problem, the subspace clustering technique has been developed, which aims at grouping the data objects into clusters based on the subsets of attributes rather than the entire data space. However, the most existing subspace clustering methods are only applicable to either numerical or categorical data, but not both. This paper, therefore, studies the soft subspace clustering of data with both of the numerical and categorical attributes (also simply called mixed data for short). Specifically, an attribute-weighted clustering model based on the definition of object-cluster similarity is presented. Accordingly, a unified weighting scheme for the numerical and categorical attributes is proposed, which quantifies the attribute-to-cluster contribution by taking into account both of intercluster difference and intracluster similarity. Moreover, a rival penalized competitive learning mechanism is further introduced into the proposed soft subspace clustering algorithm so that the subspace cluster structure as well as the most appropriate number of clusters can be learned simultaneously in a single learning paradigm. In addition, an initialization-oriented method is also presented, which can effectively improve the stability and accuracy of k-means-type clustering methods on numerical, categorical, and mixed data. The experimental results on different benchmark data sets show the efficacy of the proposed approach.},
author = {Jia, Hong and Cheung, Yiu Ming},
doi = {10.1109/TNNLS.2017.2728138},
file = {:home/ejuzovitski/Downloads/08000671.pdf:pdf},
isbn = {2016053119},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Attribute weight,categorical-and-numerical data,initialization method,number of clusters,soft subspace clustering},
number = {8},
pages = {3308--3325},
pmid = {28792907},
publisher = {IEEE},
title = {{Subspace clustering of categorical and numerical data with an unknown number of clusters}},
volume = {29},
year = {2018}
}
@inbook{doi:10.1002/9780470316801.ch1,
abstract = {Summary The prelims comprise: Motivation Types of Data and How to Handle Them Which Clustering Algorithm to Choose A Schematic Overview of Our Programs Computing Dissimilarities with the Program DAISY},
author = {Kaufman, Leonard. and Rousseeuw, Peter J.},
booktitle = {Finding Groups in Data},
chapter = {1},
doi = {10.1002/9780470316801.ch1},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufman, Rousseeuw - 2008 - Introduction.pdf:pdf},
isbn = {9780470316801},
keywords = {archeological findings,cluster analysis,interval-scaled variables,social sciences,spherical clusters},
pages = {1--67},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Introduction}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470316801.ch1},
year = {2008}
}
@article{CHEUNG20132228,
abstract = {Most of the existing clustering approaches are applicable to purely numerical or categorical data only, but not the both. In general, it is a nontrivial task to perform clustering on mixed data composed of numerical and categorical attributes because there exists an awkward gap between the similarity metrics for categorical and numerical data. This paper therefore presents a general clustering framework based on the concept of object-cluster similarity and gives a unified similarity metric which can be simply applied to the data with categorical, numerical, and mixed attributes. Accordingly, an iterative clustering algorithm is developed, whose outstanding performance is experimentally demonstrated on different benchmark data sets. Moreover, to circumvent the difficult selection problem of cluster number, we further develop a penalized competitive learning algorithm within the proposed clustering framework. The embedded competition and penalization mechanisms enable this improved algorithm to determine the number of clusters automatically by gradually eliminating the redundant clusters. The experimental results show the efficacy of the proposed approach.},
author = {Cheung, Yiu-ming and Jia, Hong},
doi = {https://doi.org/10.1016/j.patcog.2013.01.027},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/catnummix.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Categorical attribute,Clustering,Number of clusters,Numerical attribute,Similarity metric},
number = {8},
pages = {2228--2238},
title = {{Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320313000666},
volume = {46},
year = {2013}
}
@article{DBLP:journals/corr/abs-cs-0509011,
archivePrefix = {arXiv},
arxivId = {cs/0509011},
author = {He, Zengyou and Xu, Xiaofei and Deng, Shengchun},
eprint = {0509011},
journal = {CoRR},
primaryClass = {cs},
title = {{Clustering Mixed Numeric and Categorical Data: {\{}A{\}} Cluster Ensemble Approach}},
url = {http://arxiv.org/abs/cs/0509011},
volume = {abs/cs/050},
year = {2005}
}
@book{zhou2012ensemble,
author = {Zhou, Zhi-Hua},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/ensemble.pdf:pdf},
isbn = {9781439830031},
pages = {135--156},
publisher = {Chapman and Hall/CRC},
title = {{Ensemble methods: foundations and algorithms}},
year = {2012}
}
@inproceedings{Huang97clusteringlarge,
author = {Huang, Zhexue},
booktitle = {In The First Pacific-Asia Conference on Knowledge Discovery and Data Mining},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/kproto.pdf:pdf},
pages = {21--34},
title = {{Clustering large data sets with mixed numeric and categorical values}},
year = {1997}
}
@incollection{Shultz2011,
address = {Boston, MA},
author = {Shultz, Thomas R. and Fahlman, Scott E. and Craw, Susan and Andritsos, Periklis and Tsaparas, Panayiotis and Silva, Ricardo and Drummond, Chris and Ling, Charles X. and Sheng, Victor S. and Drummond, Chris and Lanzi, Pier Luca and Gama, Jo{\~{a}}o and Wiegand, R. Paul and Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and He, Jun and Jain, Sanjay and Stephan, Frank and Jain, Sanjay and Stephan, Frank and Sammut, Claude and Harries, Michael and Sammut, Claude and Ting, Kai Ming and Pfahringer, Bernhard and Case, John and Jain, Sanjay and Wagstaff, Kiri L. and Nijssen, Siegfried and Wirth, Anthony and Ling, Charles X. and Sheng, Victor S. and Zhang, Xinhua and Sammut, Claude and Cancedda, Nicola and Renders, Jean-Michel and Michelucci, Pietro and Oblinger, Daniel and Keogh, Eamonn and Mueen, Abdullah},
booktitle = {Encyclopedia of Machine Learning},
doi = {10.1007/978-0-387-30164-8_163},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Constrained Clustering.pdf:pdf},
pages = {220--221},
publisher = {Springer US},
title = {{Constrained Clustering}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-30164-8{\_}163},
year = {2011}
}
@techreport{Ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by factor of more than 100 in terms of efficiency.},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jiirg and Xu, Xiaowei},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.pdf:pdf},
keywords = {Arbitrary Shape of Clus-ters,Clustering Algorithms,Efficiency on Large Spatial Databases,Handling Nlj4-275oise},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
url = {www.aaai.org},
year = {1996}
}
@inproceedings{Shirkhorshidi2014,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.1881v2},
author = {Shirkhorshidi, Ali Seyed and Aghabozorgi, Saeed and Wah, Teh Ying and Herawan, Tutut},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-09156-3_49},
eprint = {arXiv:1101.1881v2},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shirkhorshidi et al. - 2014 - Big Data Clustering A Review(2).pdf:pdf},
isbn = {9783319091556},
issn = {16113349},
keywords = {Big Data,Clustering,MapReduce,Parallel Clustering},
number = {PART 5},
pages = {707--720},
pmid = {17707831},
publisher = {Springer, Cham},
title = {{Big data clustering: A review}},
url = {http://link.springer.com/10.1007/978-3-319-09156-3{\_}49},
volume = {8583 LNCS},
year = {2014}
}
@article{CHEN2016271,
abstract = {Most data streams encountered in real life are data objects with mixed numerical and categorical attributes. Currently most data stream algorithms have shortcomings including low clustering quality, difficulties in determining cluster centers, poor ability for dealing with outliers' issue. A fast density-based data stream clustering algorithm with cluster centers automatically determined in the initialization stage is proposed. Based on data attribute relationships analysis, mixed data sets are filed into three types whose corresponding distance measure metrics are designed. Based on field intensity-distance distribution graph for each data object, linear regression model and residuals analysis are used to find the outliers of the graph, enabling cluster centers automatic determination. After the cluster centers are found, all data objects can be clustered according to their distance with centers. The data stream clustering algorithm adopts an online/offline two-stage processing framework, and a new micro cluster characteristic vector to maintain the arriving data objects dynamically. Micro clusters decay function and deletion mechanism of micro clusters are used to maintain the micro clusters, which reflects the data stream evolution process accurately. Finally, the performances of the proposed algorithm are testified by a series of experiments on real-world mixed data sets in comparison with several outstanding clustering algorithms in terms of the clustering purity, efficiency and time complexity.},
author = {Chen, Jin-Yin and He, Hui-Hao},
doi = {https://doi.org/10.1016/j.ins.2016.01.071},
file = {:home/ejuzovitski/Downloads/1-s2.0-S0020025516300032-main.pdf:pdf},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {Data mining,Data stream clustering,Mixed attributes,Mixed distance measure metrics,Peak field intensity},
pages = {271--293},
title = {{A fast density-based data stream clustering algorithm with cluster centers self-determined for mixed data}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025516300032},
volume = {345},
year = {2016}
}
@article{doi:10.1002/widm.1057,
abstract = {Abstract Subspace clustering refers to the task of identifying clusters of similar objects or data records (vectors) where the similarity is defined with respect to a subset of the attributes (i.e., a subspace of the data space). The subspace is not necessarily (and actually is usually not) the same for different clusters within one clustering solution. In this article, the problems motivating subspace clustering are sketched, different definitions and usages of subspaces for clustering are described, and exemplary algorithmic solutions are discussed. Finally, we sketch current research directions. {\textcopyright} 2012 Wiley Periodicals, Inc. This article is categorized under: Technologies {\textgreater} Structure Discovery and Clustering},
author = {Kriegel, Hans-Peter and Kr{\"{o}}ger, Peer and Zimek, Arthur},
doi = {10.1002/widm.1057},
file = {:home/ejuzovitski/Downloads/widm.1057.pdf:pdf},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {4},
pages = {351--364},
title = {{Subspace clustering}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1057},
volume = {2},
year = {2012}
}
@article{Guha2000,
abstract = {Clustering, in data mining, is useful to discover distribution patterns in the underlying data. Clustering algorithms usually employ a distance metric based (e.g., euclidean) similarity measure in order to partition the database such that data points in the same partition are more similar than points in different partitions. In this paper, we study clustering algorithms for data with boolean and categorical attributes. We show that traditional clustering algorithms that use distances between points for clustering are not appropriate for boolean and categorical attributes. Instead, we propose a novel concept of links to measure the similarity/proximity between a pair of data points. We develop a robust hierarchical clustering algorithm ROCK that employs links and not distances when merging clusters. Our methods naturally extend to non-metric similarity measures that are relevant in situations where a domain expert/similarity table is the only source of knowledge. In addition to presenting detailed complexity results for ROCK, we also conduct an experimental study with real-life as well as synthetic data sets to demonstrate the effectiveness of our techniques. For data with categorical attributes, our findings indicate that ROCK not only generates better quality clusters than traditional algorithms, but it also exhibits good scalability properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Guha, Sudipto and Rastogi, Rajeev and Shim, Kyuseok},
doi = {10.1016/S0306-4379(00)00022-3},
eprint = {arXiv:1011.1669v3},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guha, Rastogi, Shim - 2000 - Rock a robust clustering algorithm for categorical attributes.pdf:pdf},
isbn = {0-7695-0071-4},
issn = {03064379},
journal = {Information Systems},
pmid = {25246403},
title = {{Rock: a robust clustering algorithm for categorical attributes}},
year = {2000}
}
@article{Ng2002,
abstract = {Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. To this end, this paper has three main contributions. First, we propose a new clustering method called CLARANS, whose aim is to identify spatial structures that may be present in the data. Experimental results indicate that, when compared with existing clustering methods, CLARANS is very efficient and effective. Second, we investigate how CLARANS can handle not only points objects, but also polygon objects efficiently. One of the methods considered, called the IR-approximation, is very efficient in clustering convex and nonconvex polygon objects. Third, building on top of CLARANS, we develop two spatial data mining algorithms that aim to discover relationships between spatial and nonspatial attributes. Both algorithms can discover knowledge that is difficult to find with existing spatial data mining algorithms. Index Terms-Spatial data mining, clustering algorithms, randomized search, computational geometry.},
author = {Ng, Raymond T and Han, Jiawei},
doi = {10.1109/TKDE.2002.1033770},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Han - Unknown - CLARANS A Method for Clustering Objects for Spatial Data Mining.pdf:pdf},
journal = {IEEE Transaction on Knowledge and Data Engineering},
title = {{CLARANS : A method for clustering objects for}},
year = {2002}
}
