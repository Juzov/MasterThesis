\documentclass[../report.tex]{subfiles}

\begin{document}
\chapter{Discussion}

\section{Quantitative results}
That K-means would marginally beat out all soft-subspace clustering algorithms was unexpected and surprising. There could be various reasons for the occurrence, they all start with the dataset.

The properties of the dataset could have made it less suitable for SSC algorithms. One possibility is that the dataset had in a sense already had its features selected beforehand, the 160 neuron weights selected as features were already picked from a larger quantity of neurons based in the neural network. SSC algorithms could theoretically still boost the performance of clusters after the feature sampling, by finding the unique subspaces of each cluster, but they did not. There was no significant improvement of purity performance with LEKM, and a downgrade in performance occurred with the usage of EWKM.

The fairly popular soft subspace clustering algorithm of EWKM performed poorly on the dataset. One reason was due to the single feature dominance. However, another problem that arose was the restriction of values $\gamma$ that could be used in order to avoid immediate convergence. This problem was not mentioned in the paper introducing the algorithm \cite{Jing2007}, in the paper the weights were supposed to be of a normal distribution, with some features more and less important per cluster.

FSC had the accuracy scores increase with a larger $\beta$. A larger $\beta$ did mean more uniform feature weights. The result would for that reason, indicate that important features are disregarded as irrelevant when smaller values of $\beta$ is used. The algorithm failed in that sense, to find the important features, and could not reduce the dimensionality of the dataset. Where \textit{FSC} did not need to reduce the dimensions the score improved.

LEKM, which created feature weights that were normally distributed on each cluster, performed similarly to K-means. It had one advantage to K-means. The generated feature weights of each cluster had given less features importance when calculating the distance between data-object and cluster center. That indicates that clusters can be dimensionally reduced to a smaller subspace without impacting the accuracy negatively unlike \textit{FSC}. This shows that LEKM can still be used for the problems high-dimensionality creates in cluster analysis and could potentially cluster even larger datasets.

Finding the correct parameter for the SSC algorithms ended up being non-trivial with an external evaluation of purity based on the genre label. \textit{Genres} is not the ideal candidate for an external validity measure, and so tuning the parameters could have led to the right parameter for the SSC-algorithms being missed. The problem this thesis had with tuning the parameters of the algorithm show the problem of tuning soft subspace clustering algorithms on a new dataset. Internal indices such as silhouette have to be manipulated with the cluster specific asymmetric distance measure, while an external criterion used here, leave things left to be desired.

That the highest value of $k$ would have the highest purity value was not unexpected as smaller clusters tend to allow for higher purity accuracy. The $F_1$ score could have been more suitable for the task.

\section{Qualitative results}
The Evaluation scores of the blind-test indicates that the clusters generated by the algorithms could be used to create playlists. This is reflected by the general quality scores of the algorithms. The audio similarity was through discussions and metrics fairly high along all clusters. While, as mentioned some songs do not fit the general theme of the cluster due to cultural differences such occurrences could be tuned out by a composer generating a playlist through the use of filters.

It can therefore be stated that the audio features selected in the preprocessing were indeed enough to cluster to songs, and the algorithms were indeed capable of clustering the dataset. However, to improve the cultural similarity score one should consider adding features such as origin year, and, country of origin.

Generated clusters from the algorithms performed much worse than the playlist counterpart for the \textit{Playlist uniqueness} index. All three sources saw a drop-off compared to the \textit{General Quality} category. Notable was that playlists were rated over a value of five, when they were in fact, already available on the platform. The ratings given do not reflect the discussion the team of playlist composers had as they indicated that some clusters had a good blend and hard to replicate without hearing the cluster. Based on this, it could be stated that there was a misunderstanding of what the index meant. Based on the discussions alone, the belief is that the novelty criteria had been met.

K-means performed slightly better than LEKM in every category. The results indicate that K-means perform at least as good as LEKM, which for the most part corresponds to the results of purity. For statistical confidence that K-means performs better on the dataset than LEKM, a larger cluster sample size would have been needed.


\section{Future Work}
Adding additional features to the feature-space of the given dataset could have seen the results of the algorithms improve. The cultural similarity --- where the algorithms struggled compared to the playlists --- could have been helped by metadata such as origin year. To use more of the metadata e.g. \textit{artist} (represents songs with the same artist), categorical features would have been needed to be used. There are two main ways to tackle the problem of mixed data with soft-subspace clustering. Modify the given algorithm's distance measurement to adhere to mixed data or use a mixed data clustering algorithm. The former is not a trivial task, and so the latter could be a better choice. The WOCIL algorithm mentioned in \cite{Jia2018} would then be a suitable choice the for mixed data clustering on the dataset.

All the given results are based on the $50k$ sample of the larger dataset. Scaling the clustering process to the larger $50$-million set should impact \textit{k-means} negatively. Feature reduction could at that point become more essential for the clustering process, and so, SSC algorithms could become more useful.

Moving to a larger dataset requires more processing. Using a single thread on a single core is not efficient. An idea would instead to scale the algorithms across multiple computer nodes using a distributed framework. The distributed implementation of the algorithms would require the code to be rewritten to fit the capabilities of the framework.

Algorithms based on k-means are non-deterministic due to the sampling of initial centers. Finding and using suitable sampling techniques as mentioned in \cite{Gan2016} specifically made for subspace-clustering could improve the results of the soft-subspace clustering methods.

Many of the internal validation indices that exist for clustering analysis takes account both intra-cluster distance and inter-cluster distance --- An ideal clustering should have clusters far between. The chosen SSC algorithms were all based on intra-cluster distance. No account was taken on the distance between clusters. A natural step to improve performance would be to find an algorithm which takes both conditions to account.
\end{document}

