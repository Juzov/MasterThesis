\documentclass[../report.tex]{subfiles}

\begin{document}
\chapter{Discussion}
The objective of this thesis was to answer whether clustering algorithms can be used to find high-quality novel musical themes. The problem statement asked if there was a performance difference between traditional methods (represented through \textit{k-means}) and \textit{SSC-algorithms} on the given high-dimensional dataset.

In this paper, k-means and SSC algorithms have been compared for a given musical high-dimensional dataset. Three SSC algorithms EWKM, FSC and LEKM
were selected for the comparison with k-means. The given dataset was preprocessed to only include numerical audio features  --- based on neuron weights of a trained supervised neural network --- and normalized using the \textit{z-score}. The dataset was sampled to a size of $50k$. Two external evaluation indices were set up, a \textit{purity} measurement based on a genre feature of the dataset, and a blind test involving a panel of expert judges. The purity measurement was used to find suitable input parameters --- algorithm-specific ones, e.g. $\beta$ and \textit{k} (given a choice of 50, 100, 500). Additionally, the algorithm with the best purity score (LEKM) was used as the SSC-algorithm for comparison with k-means in the blind test. The blind-test was composed of clusters of different sources: Playlist, k-means, and LEKM. The expert judges --- playlist composers --- had to evaluate the clusters based on multiple criteria.

\begin{color}{modified}
Resulting purity scores of k-means and the best performing clustering algorithm LEKM show a purity difference of less than $0.1\%$ with optimal parameters on the sample dataset. EWKM performed significantly worse than other algorithms with regards to purity. As for the expert evaluation, it was shown that a null hypothesis --- $H_0 = [ \mu_{\textit{pl}} = \mu_{\textit{SSC}} = \mu_{\textit{k-means}} ]$ --- cannot be rejected based on the evaluation scores given by the playlist composers that were subjected to the test.
\end{color}

% The methods were chosen to answer the dataset specific objective, of whether clustering algorithms can be used to find high-quality novel musical themes. The validation is based on unique properties of the dataset, and the resource of expert judges. Additionally, the research methods were selected to shed light on SSC-algorithms, and evaluate the type for a real-world high-dimensional dataset in comparison to \textit{k-means}.

% The rest of the discussion is divided into \textit{General Discussion} and \textit{Evaluation Discussion}. General Discussion refers to the discussion of \nameref{section:general} (\cref{section:general}), i.e. how algorithms behaved for different parameter selections, both in terms of feature weight distribution and purity. Evaluation Discussion refers to the discussion of \nameref{section:external} (\cref{section:external}), i.e. how LEKM and k-means faired in comparison to playlists based on evaluation criteria. Both sections also critique the method selection of generating the given results.

\section{General Discussion}
\begin{color}{modified}
Based on previous work \cite{Jing2007, Gan2006}, a high-dimensional dataset such as the given dataset, should see SSC-algorithms improve clustering performance by lessening the weight of irrelevant features through automatic feature weighting. The results of this paper did not show improvements in using a SSC-algorithm on the dataset: there was negligible difference between k-means and the best performing SSC algorithm. The results are unexpected as SSC algorithms are made for high-dimensional datasets.
\end{color}

Properties of the dataset could have made it less suitable for SSC algorithms, a reason for the difference between the results obtained and what was shown in \cite{Jing2007}. One possibility is that the dataset had in a sense already had its features selected beforehand: the 160 weights selected as features, were already sampled from the set of weights in the neural network. SSC algorithms could theoretically have boosted the performance even with the feature sampling, by finding the unique subspaces of each cluster, but it did not. There was no significant improvement of purity performance with LEKM, and a downgrade in performance occurred with the usage of EWKM. It can be concluded that SSC-algorithms are not beneficiary for the given dataset. On the other-hand, it can be questioned whether the dataset as it was pre-sampled is a good representation of a high-dimensional dataset. The results could have been more representative if another publicly available dataset was used. As it stands now, there is a limitation on how the results can be extended to other datasets.

The popular soft-subspace clustering algorithm of EWKM performed poorly on the dataset. The possible choices of $\gamma$ were restricted to lower values in order to avoid immediate convergence. For values that did not immediately converge, the algorithm suffered from single feature-weight dominance in clusters, making it hard to represent the clusters in an impactful manner. The algorithm had clearly failed the task of feature reduction, which in turn had resulted in worse purity scores. This shows that the algorithm should not be used for just any numerical high-dimensional dataset.

The problem of dominant weights was not mentioned in the paper introducing the EWKM algorithm \cite{Jing2007}.  The feature weights were supposed to be of a normal distribution, with some features more and less important per cluster.

FSC had the accuracy scores increase in comparison to EWKM. The best scores were found with an increasingly large $\beta$. A larger $\beta$ resulted in feature weights becoming more uniform in each cluster. What essentially FSC offered in feature weighting, was disregarded when selecting larger values, i.e. $\beta=15,30$. From this standpoint the best (and largest) $\beta$ was not true to subspace clustering, there was no significant feature weight dispersion, but for this dataset it was not necessary according to the purity scores. A reason could be that the algorithm failed to determine the important features from the unimportant features, and clusters were given subspaces of actually unimportant features. For this case, it would be better to allow all features the same weight. This would also mean that the algorithm failed in terms of feature reduction.

LEKM, which created feature weights that were normally distributed on each cluster, performed similarly to k-means. It had one advantage to k-means. A smaller fraction of features had an impact when calculating the distance between data-object and cluster center, when using the best $\gamma$. LEKM allowed more features to be of a significant weight in comparison to EWKM, which shows the benefit of using log-transformed distances. Clusters could be dimensionally reduced to a smaller subspace without impacting the accuracy negatively unlike FSC. The algorithm is based on this, suitable for this dataset and other datasets of high-dimensionality.

%log distances
The chosen external validation index --- Purity with \textit{Genres} as a ground truth --- promotes the choice of values based on genre homogeneity. It is probable that more homogeneous genre clusters are in general of better quality than those with more genre dispersion. What it failed to do is to promote genre novelty --- new types of genres based on unique properties. A better choice of parameters could have been found if the index had taken the novelty into account, however, genre was the only label provided for the dataset.

That the highest value of $k$ (500) would have the highest purity value was not unexpected --- smaller clusters tend to allow for higher purity accuracy. The $F_1$ score could have avoided such bias, but as mentioned in the background, would involve more complex computations. The choice of candidate $k$'s, which were decided by the author to be $50$, $100$, and $500$, could have been extended with more options for the higher probability of finding the optimal $k$. This suggested extension would have exponentially increased the parameter selection process, as each \textit{k} requires an array of algorithm-specific parameter values to be tested.

\begin{color}{modified}
An intentional limitation was to run algorithms once for each parameter combination on the sample dataset. There were two problems of this limitation: We could not with statistical significance determine whether there was a purity difference between algorithms, as performance differences could be caused by initial cluster sampling. As for the second problem, we assumed that the parameters chosen are representative for the whole dataset when no such assumption can be made.
\end{color}

% In the current test hyperparameters are tuned by the purity of a sample dataset. It is also evaluated on the same dataset subset. To understand if the sample scores and parameter selections are representative on the whole dataset it would be beneficial to include a evaluation test including data outside of the chosen sample. And so the choice of a sample to cluser on becomes a limitation of the report.

% Alogirthms were only ran once on the sample dataset. This was doe intentionally due to time ... would need. This forced 
% Another restriction that impactated the usefulness of the purity resulting scores was that algorithms were only ran once. This was done intentionally, due to the time that running the algorithms multiple times on multiple parameters would need. As they were only ran once, statistical test could not be used to find if the results had any significance.



% but is more computationally heavy. Internal indices --- such as the average silhouette coefficient --- are an option, but are not based on the 

% Finding the correct parameter for SSC algorithms ended up being non-trivial as the external validation index --- purity based on \textit{Genres}. We did want to find novel \textit{Genres}, that had different characteristics to \textit{Genres} is not the ideal candidate for a ground truth when  and so tuning the parameters could have led to the right parameter for the SSC-algorithms being missed. The problem this thesis had with tuning the parameters of the algorithm show the problem of tuning soft subspace clustering algorithms on a new dataset. Internal indices such as silhouette have to be manipulated with the cluster specific asymmetric distance measure, while an external criterion used here, leave things left to be desired.

\section{Evaluation Discussion}
The evaluation scores of the blind-test indicate that the clusters generated by the algorithms could be used to create playlists. This is reflected by the general quality scores of the algorithms. The audio similarity was through discussions and metrics fairly high along all clusters, while some songs did not fit the general theme of the cluster due to cultural differences, such occurrences could be tuned out by allowing composers to filter out songs by artist or genre. It can therefore be stated that the audio features selected in the preprocessing were indeed enough to cluster the songs, and the algorithms were indeed capable of clustering the dataset. However, to improve the cultural similarity score one should consider adding features such as origin year, and, country of origin. In terms of mean score a difference is shown between k-means and LEKM, k-means has an overall higher mean scores than LEKM.

% Generated k-means clusters had a higher mean than existing playlists and LEKM for \textit{Playlist Uniqueness}. The difference could be answered by the inability of the evaluaters to differentiate \textit{Playlist Uniqueness} and \textit{General Quality}. Based on discussions with the composers playlist uniqueness in a cluster were rated lower, when the General Quality was deemed bad.blind-test.

\begin{color}{modified}
The output of the single-factor ANOVA tests yield a result that cannot reject a null hypothesis for any of the given evaluation criteria. It cannot be statistically determined if there is a significant score difference between the clusters of the different sources for the chosen criteria. The inability to reject the null hypothesis can be seen as a positive result: LEKM and k-means are performing similarly to existing playlists. The results are however, contradicting expected behaviour: The null hypothesis is not rejected for \textit{Cultural Similarity}, yet the algorithms are only trained on audio-based features. In other words a difference is expected for existing playlists and the algorithms in regards to \textit{Cultural Similarity}.
\end{color}


% Generated clusters from the algorithms performed much worse than the playlist counterpart for the \textit{Playlist uniqueness} index. All three sources saw a drop-off compared to the \textit{General Quality} category. Notable was that playlists were rated over a value of five, when they were in fact, already available on the platform. The ratings given do not reflect the discussion the team of playlist composers had as they indicated that some clusters had a good blend and hard to replicate without hearing the cluster. Based on this, it could be stated that there was a misunderstanding of what the index meant. Based on the discussions alone, the belief is that the novelty criteria had been met.
% Generated clusters from the algorithms performed much worse than the playlist counterpart for the \textit{Playlist uniqueness} index. All three sources saw a drop-off compared to the \textit{General Quality} category. Notable was that playlists were rated over a value of five, when they were in fact, already available on the platform. The ratings given do not reflect the discussion the team of playlist composers had as they indicated that some clusters had a good blend and hard to replicate without hearing the cluster. Based on this, it could be stated that there was a misunderstanding of what the index meant. Based on the discussions alone, the belief is that the novelty criteria had been met.

% To summarize the \nameref{section:external}, k-means performed slightly better than LEKM in every category. The results indicate that k-means perform at least as good as LEKM, which for the most part corresponds to the results of purity. For statistical confidence that k-means performs better on the dataset than LEKM, a larger cluster sample size would have been needed.
\section{Ethics, Sustainability and \newline Societal Aspects}
Ethical concerns are commonplace in automatic categorization. In the context of the song dataset, songs from a specific cluster could be used to generate a playlist for a popular streaming platform. That cluster might have it pivotal that an artist's country of origin is a specific country. Artists from another country of origin which otherwise fit the cluster, are disregarded due to a seemingly, artificial wall. As such, artists from less established markets can become invincible for that playlist resulting in loss of revenue. Ethically, it is questionable whether country of origin should have merit or not. From a cluster quality standpoint excluding some songs, for a better precision is a worthy trade-off. In this thesis only audio data is processed for clustering, which forces the algorithm to only compare songs based on audio.

% Clustering music does not have a large societal aspect, apart from the musical industry. From this standpoint, companies with a music library as those can be interested in the work. 
High-dimensional datasets are frequent in the real-world. The comparison between the lesser known SSC-algorithms and k-means on the given dataset are beneficial for researchers and data-scientists looking for the possibility to cluster high-dimensional data. This thesis provides results on a new real-world dataset, without any bias towards a self-published algorithm. Additionally, problems of using the given algorithms on the dataset are discussed, which helps highlight possible problems on similar datasets.

\section{Future Work}
Adding additional features to the feature-space of the given dataset could have seen the results of the algorithms improve. The cultural similarity --- where the algorithms struggled compared to the playlists --- could have been helped by metadata such as origin year. To use more of the metadata e.g. \textit{artist} (represents songs with the same artist), categorical features would have been needed to be used. There are two main ways to tackle the problem of mixed data with soft-subspace clustering. Modify the given algorithm's distance measurement to adhere to mixed data or use a mixed data clustering algorithm. The former is not a trivial task, and so the latter could be a better choice. The WOCIL algorithm mentioned in \cite{Jia2018} would then be a suitable choice the for mixed data clustering on the dataset.

All the given results are based on the $50k$ sample of the larger dataset. Scaling the clustering process to the larger $50$-million set should impact k-means negatively. Feature reduction could at that point become more essential for the clustering process, and so, SSC algorithms could become more useful.

Moving to a larger dataset requires more processing. Using a single thread on a single core is not efficient. An idea would instead be to scale the algorithms across multiple computer nodes using a distributed framework. The distributed implementation of the algorithms would require the code to be rewritten to fit the capabilities of the framework.

Algorithms based on k-means are non-deterministic due to the sampling of initial centers. Finding and using suitable sampling techniques as mentioned in \cite{Gan2016} specifically made for subspace-clustering could improve the results of the soft-subspace clustering methods.

Many of the internal validation indices that exist for clustering analysis take both intra-cluster distance and inter-cluster distance into account --- an ideal clustering should have clusters far between. The chosen SSC algorithms were all based on intra-cluster distance. No account was taken on the distance between clusters. A natural step to improve performance would be to find an algorithm which takes both conditions into account.

Finally, to allow the results to be statistically tested, algorithms should be run multiple times for each possible candidate combination on one or more representative samples of the dataset, and judges should be given more clusters to rate.

\end{document}

