\documentclass[../report.tex]{subfiles}

\begin{document}
\chapter{Discussion}

In this paper, \textit{K-means} and SSC Algorithms have been compared for a given high-dimensional dataset. Three SSC algorithms \textit{(EWKM, FSC, LEKM)} were selected for the comparison with K-means. The given dataset was preprocessed to only include numerical audio features --- based on neuron weights of a trained supervised neural network --- and normalized using the \textit{z-score}. Two external evaluation indices were set up, a \textit{purity} measurement based on a genre feature of the dataset, and a blind test involving a panel of expert judges. The purity measurement was used to find suitable input parameters --- algorithm specific ones, e.g. $\beta$ and \textit{k} (given a choice of 50, 100, 500). Additionally, the algorithm with the best purity score (LEKM) was used as the SSC-algorithm for comparison with K-means in the blind test. The blind-test was composed of clusters of different sources: Playlist, K-means, and LEKM. The expert judges --- playlist composers --- had to evaluate the clusters based on multiple criteria.

The methods were chosen to answer the dataset specific objective, of whether clustering algorithms can be used to find high-quality novel musical themes. The validation is based on unique properties of the dataset, and the resource of expert judges. Additionally, the research methods were selected to shed light on SSC-algorithms, and evaluate the type for a real-world high-dimensional dataset in comparison to \textit{K-means}.

The rest of the discussion is divided into \textit{General Results} and \textit{Evaluation Results}. \textit{General Results} refers to the discussion around how algorithms behaved for different parameter selections, both in terms of feature weight distribution and purity. \textit{Evaluation Results} refers to the discussion of the blind-test results.

\section{General results}
Based on previous papers \cite{Jing2007, Gan2006}, a high-dimensional dataset such as this one, should see SSC-algorithms improving clustering performance by lessening the weight of irrelevant features through automatic feature weighting. The \textit{General Results} showed negligible difference between K-means and the best performing SSC algorithm. This can be seen as a surprising result, after all, SSC algorithms are made for high-dimensional datasets.

Properties of the dataset could have made it less suitable for SSC algorithms, a reason for the difference between the results obtained and what was shown in \cite{Jing2007}. One possibility is that the dataset had in a sense already had its features selected beforehand: the 160 neuron weights selected as features were already picked from a larger quantity of neurons based in the neural network. SSC algorithms could theoretically had boosted the performance even with the feature sampling, by finding the unique subspaces of each cluster, but it did not. There was no significant improvement of purity performance with LEKM, and a downgrade in performance occurred with the usage of EWKM. It can be concluded that SSC-algorithms are not beneficiary for the given dataset. On the other-hand, it can be questioned whether the dataset as it was pre-sampled is a good representation of a high-dimensional dataset. The results could have been more representative if another publicly available dataset was used. As it stands now, there is a limitation on how the results can be extended to other datasets.

The fairly popular soft subspace clustering algorithm of EWKM performed poorly on the dataset. The possible choices of $\gamma$ was restricted to lower values in order to avoid immediate convergence. For values that did not immediately converge, the algorithm suffered from single feature-weight dominance in clusters, making it hard to represent the clusters in an impactful manner. The algorithm had clearly failed the task of feature reduction, which in turn had resulted in worse \textit{purity} scores. This shows that the algorithm should not be used for just any numerical high-dimensional dataset.

The problem of dominant weights was not mentioned in the paper introducing the algorithm \cite{Jing2007}, in the paper the weights were supposed to be of a normal distribution, with some features more and less important per cluster.

FSC had the accuracy scores increase in comparison to EWKM. The best scores were found with with an increasingly large $\beta$. A larger $\beta$ resulted in feature weights becoming more uniform in each cluster. What essentially $FSC$ offered in feature weighting, was disregarded when selecting larger values, i.e. $\beta=15,30$. From this standpoint the best (and largest) $\beta$ was not true to subspace clustering, there was no significant feature weight dispersion, but for this dataset it was not necessary according to the purity scores. A reason could be that the algorithm failed to determine the important- from the unimportant-features, and clusters were given subspaces of actually unimportant features. For this case, it would be better to allow all features the same weight. This would also mean that the algorithm failed in terms of feature reduction.

LEKM, which created feature weights that were normally distributed on each cluster, performed similarly to K-means. It had one advantage to K-means. A smaller fraction of features had an impact when calculating the distance between data-object and cluster center, when using the best $\gamma$. Unlike EWKM, features were normally distributed, which shows the benefit of using log-transformed distances. Clusters could be dimensionally reduced to a smaller subspace without impacting the accuracy negatively unlike \textit{FSC}. The algorithm is based on this, suitable for this dataset and other datasets of high-dimensionality.

%log distances
The chosen external validation index --- Purity with \textit{Genres} as a ground truth --- promotes the choice of values based on \textit{genre} homogeneity. It is probable, that more homogeneous \textit{genre} clusters are in general of better quality than those with more \textit{genre} dispersion. What it failed to do is to promote genre novelty --- new types of genres based on unique properties. There could have been a better choice of parameters if the index could have taken the novelty into account, however, \textit{genre} was the only label provided for the dataset.

That the highest value of $k$ (500), would have the highest purity value was not unexpected --- smaller clusters tend to allow for higher purity accuracy. The $F_1$ score could have been to avoid such bias, but as mentioned in the background, would involve more complex computations. The choice of candidate $k$'s, which were decided by the author to be $50$, $100$, and $500$, could have been extended with more options for the higher probability of finding the optimal $k$. This suggested extension would have exponentially increased the result generation process, as each \textit{k} requires an array of algorithm-specific parameter values to be tested.

% but is more computationally heavy. Internal indices --- such as the average silhouette coefficient --- are an option, but are not based on the 

% Finding the correct parameter for SSC algorithms ended up being non-trivial as the external validation index --- purity based on \textit{Genres}. We did want to find novel \textit{Genres}, that had different characteristics to \textit{Genres} is not the ideal candidate for a ground truth when  and so tuning the parameters could have led to the right parameter for the SSC-algorithms being missed. The problem this thesis had with tuning the parameters of the algorithm show the problem of tuning soft subspace clustering algorithms on a new dataset. Internal indices such as silhouette have to be manipulated with the cluster specific asymmetric distance measure, while an external criterion used here, leave things left to be desired.

\section{Evaluation results}
The evaluation scores of the blind-test indicates that the clusters generated by the algorithms could be used to create playlists. This is reflected by the general quality scores of the algorithms. The audio similarity was through discussions and metrics fairly high along all clusters. While as mentioned, some songs do not fit the general theme of the cluster due to cultural differences such occurrences could be tuned out by a composer generating a playlist through the use of filters.

It can therefore be stated that the audio features selected in the preprocessing were indeed enough to cluster the songs, and the algorithms were indeed capable of clustering the dataset. However, to improve the cultural similarity score one should consider adding features such as origin year, and, country of origin.

Generated K-means clusters performed better than playlists on \textit{Playlist Uniqueness}, while \textit{LEKM} clusters performed worse. K-means can therefore be seen as meeting the criteria for novel clusters. LEKM on the other hand, can not be confirmed to create novel clusters, and would require additional testing on more clusters. Based on discussions with playlist composers and how they rated clusters, \textit{Playlist Uniqueness} in a cluster was rated lower, if the \textit{General Quality} was deemed bad. The explanation of the criteria could skewed the results, and could have been explained better by the author before the blind-test.

% Generated clusters from the algorithms performed much worse than the playlist counterpart for the \textit{Playlist uniqueness} index. All three sources saw a drop-off compared to the \textit{General Quality} category. Notable was that playlists were rated over a value of five, when they were in fact, already available on the platform. The ratings given do not reflect the discussion the team of playlist composers had as they indicated that some clusters had a good blend and hard to replicate without hearing the cluster. Based on this, it could be stated that there was a misunderstanding of what the index meant. Based on the discussions alone, the belief is that the novelty criteria had been met.
% Generated clusters from the algorithms performed much worse than the playlist counterpart for the \textit{Playlist uniqueness} index. All three sources saw a drop-off compared to the \textit{General Quality} category. Notable was that playlists were rated over a value of five, when they were in fact, already available on the platform. The ratings given do not reflect the discussion the team of playlist composers had as they indicated that some clusters had a good blend and hard to replicate without hearing the cluster. Based on this, it could be stated that there was a misunderstanding of what the index meant. Based on the discussions alone, the belief is that the novelty criteria had been met.

To summarize the \textit{Evaluation Results}, K-means performed slightly better than LEKM in every category. The results indicate that K-means perform at least as good as LEKM, which for the most part corresponds to the results of purity. For statistical confidence that K-means performs better on the dataset than LEKM, a larger cluster sample size would have been needed.

\section{Future Work}
Adding additional features to the feature-space of the given dataset could have seen the results of the algorithms improve. The cultural similarity --- where the algorithms struggled compared to the playlists --- could have been helped by metadata such as origin year. To use more of the metadata e.g. \textit{artist} (represents songs with the same artist), categorical features would have been needed to be used. There are two main ways to tackle the problem of mixed data with soft-subspace clustering. Modify the given algorithm's distance measurement to adhere to mixed data or use a mixed data clustering algorithm. The former is not a trivial task, and so the latter could be a better choice. The WOCIL algorithm mentioned in \cite{Jia2018} would then be a suitable choice the for mixed data clustering on the dataset.

All the given results are based on the $50k$ sample of the larger dataset. Scaling the clustering process to the larger $50$-million set should impact \textit{k-means} negatively. Feature reduction could at that point become more essential for the clustering process, and so, SSC algorithms could become more useful.

Moving to a larger dataset requires more processing. Using a single thread on a single core is not efficient. An idea would instead to scale the algorithms across multiple computer nodes using a distributed framework. The distributed implementation of the algorithms would require the code to be rewritten to fit the capabilities of the framework.

Algorithms based on k-means are non-deterministic due to the sampling of initial centers. Finding and using suitable sampling techniques as mentioned in \cite{Gan2016} specifically made for subspace-clustering could improve the results of the soft-subspace clustering methods.

Many of the internal validation indices that exist for clustering analysis takes account both intra-cluster distance and inter-cluster distance --- An ideal clustering should have clusters far between. The chosen SSC algorithms were all based on intra-cluster distance. No account was taken on the distance between clusters. A natural step to improve performance would be to find an algorithm which takes both conditions to account.
\end{document}

