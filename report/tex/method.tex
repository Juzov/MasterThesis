\documentclass[../report.tex]{subfiles}

\begin{document}
\chapter{Method}
\section{Dataset}

% Clustering analysis will be made on a real-world-, high-dimensional- and, mixed-dataset.
Clustering analysis is made on a real-world-, high-dimensional- and, mixed-dataset. The dataset is a set of \textit{songs} extracted from a music database. Each song is a vector of features --- each feature describes the song in a unique way. The vector consists of two different types of features; General metadata and metadata generated from an audio embedding.

The general metadata include attributes that identify the song without resulting to audio analysis. They include: Album, Artist, Title, Year of Origin. The type also includes BPM as well as .... The general metadata consists of both numerical and categorical data --- features stored as strings can be converted to categorical data. Furthermore, certain features are stored dynamically i.e. a song can have multiple genres or no genre at all.

The second feature type is a 160-dimensional vector embedding --- with numerical values ranging through -1 and 1 --- that characterizes the song through audio analysis. The embedding is generated through a neural network of auto-encoders, trained on the Mel-frequency cepstrum coefficients(MFCC)\cite{Paliwal2010}. The feature type is anonymous to humans, i.e. what exactly dimension 5 represents is unknown.

\subsection{MFCC}
To understand MFCC we need to first understand how humans perceive sound and how speech is created. Humans are more sensitive to frequency differences at lower frequencies compared to higher frequencies. The Mel-scale takes the differences in sensitivity into account by creating a logarithmic scale. Human speech is generated by the shape of the vocal tract. The shape determines what sound is created.
A power spectrum describes how much power is in the frequency component of a signal. The envelope of the power spectrum can be used to represent the shape of the vocal tract of the given audio. An MFCC represents the envelope \cite{Paliwal2010}. With the shape of the tract represented, generated phonemes can be determined of the vocals of the audio.

\section{Pre-Processing of dataset}
The choice of numerical clustering methods restricts the dataset for clustering to only include numerical data. As such, only the numerical features of the general metadata could be used in addition to the audio embedding. The decision was made to restrict the dataset in the beginning to only the audio-embedding excluding all of the general features as there was a hypothesis that the audio embedding would be result in good enough clusters.

The audio-embedding features were extracted from the original dataset and used as the input for the clustering algorithms. Before sending in the data, it was standardized using the z-score method.

\section{Algorithm Implementation}
\section{EWKM Implementation}
The chosen EWKM algorithm is available in $R and CSRAN$ through the $wskm$ package. The source code is available for the public on github. The algorithm is implemented in $C$ and wrapped for $R$.

The implementation differs slightly in how $\lambda$ is calculated from the algorithm shown in ... It includes additional normalization steps. Eq .. shows how $\lambda$ is updated in the given implementation. 

% restarts?

Due to the thesis author's familiarity with Python and Pandas DataFrame, the code was re-wrapped for Python using numpy-C-API --- allowing for the dataset to be sent in to the algorithm as a numpy array. The C-code was tweaked to not be dependent on $R's unif_rand() function$. The wrapping can be found in the attachments....

\section{K-means Implementaion}
K-means is a widely available algorithm for Python. The thesis used the PyCluster implementaion of $K-means$. The implementaion was created in $C++$ and wrapped for Python.

\section{Initialization and Parameter Tuning}


$\gamma$ is an additional parameter of EWKM. A too high $\gamma$ results in a total entropy larger than the total distance. A $\gamma$ set too low resulted in slow convergence and a high cost function. The parameter was tuned by iteratively running the clustering method on multiple $\gamma$'s and choosing the $\gamma$ which results in the lowest cost function.



\end{document}

