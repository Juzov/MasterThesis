\documentclass[../report.tex]{subfiles}
\begin{document}
\chapter{Method}
% \section{Method Design Overview}
In order to generate results and compare the three different SSC algorithms with k-means, a method design was planned and executed. The method was divided into the step of \textit{Preprocessing}, \textit{Implementation}, \textit{Parameter Selection} and \textit{External Evaluation}.

% \begin{figure}[h!]
%   \begin{center}
%     \includegraphics[width=0.4\textwidth]{Design}
%   \end{center}
%   \caption{Method Design}\label{fig:method}
% \end{figure}
\section{Dataset}
\label{sec:dataset}
% Clustering analysis will be made on a real-world-, high-dimensional- and, mixed-dataset.
The given dataset is a real-world, high-dimensional mixed-dataset. It is a set of \textit{song objects} extracted from an internal music database. There are in total $5\cdot10^7$ songs in the dataset. Each song is a vector of features where different features describe the song through different song properties. The vector consists of two types of features, general metadata and audio-based features.

The general metadata includes attributes that predominantly identify the song independent of audio analysis. Features of the type includes \textit{Album} (string), \textit{Artist} (string), \textit{Title} (string) and \textit{Year of Origin} (ordinal). The type also includes \textit{BPM} (numerical, zero to around 200) and \textit{Energy-feel} (ordinal, one to ten). These features include both numerical and categorical features --- features stored as strings can be converted to categorical data.

The core features are the audio-based features, and are derived from a supervised neural network. They are a sample of the neuron weights in a network trained to determine the genre of a song. Each song consists of an audio embedding --- devised from an audio spectrogram --- and a \textit{genre} label --- used to train the network.

Our dataset includes the genre feature. There are $33$ possible genres for the songs in the dataset.






% For our dataset there is 


% So, the reason for defining the features as audio based is due to audio data used to train the network.


% They are a sample of the neural weights of a trained supervised neural network.

% The source of the features are a trained supervised neural network. The input of the network are audio embeddings of songs --- devised from an audio spectrogram --- that have a predetermined genre. The genre is utilized as a label to train the network for the task of classifying the genre of an audio embedding. In total there are $33$ different genres a song can be labeled as.


% They represent the song through its audio properties. There in


% They represent the song with features based on audio properties of a song. There are 160 of these features, together they can be seen as 160-dimensional vector where each dimension represents a feature.


% Audio features of our dataset are a representative sample of neuron weights in the trained supervised neural network. The genre label is also a feature that exist in a smaller subset of our given dataset.

% \begin{color}{red}{
% \subsection{MFCC}
% To understand MFCC we need to first understand how humans perceive sound and how speech is created. Humans are more sensitive to frequency differences at lower frequencies compared to higher frequencies. The Mel-scale takes the differences in sensitivity into account by creating a logarithmic scale. Human speech is generated by the shape of the vocal tract. The shape determines what sound is created.
% A power spectrum describes how much power is in the frequency component of a signal. The envelope of the power spectrum can be used to represent the shape of the vocal tract of the given audio. An MFCC represents the envelope \cite{Paliwal2010}. With the shape of the tract represented, generated phonemes can be determined of the vocals of the audio.
% }\end{color}


\section{Preprocessing}
The amount of dataset objects and features were reduced in the preprocessing stage for various reasons. For the ease of testing --- reducing computation time --- the original dataset of songs was sampled and reduced to a size of $5 \cdot 10^4$.

The thesis focuses on the problem of clustering high-dimensional data. Mixed-data SSC algorithms are not widely available online, and so the choice was to use the numerical clustering methods of EWKM, LEKM, FSC. This forced the exclusion of features that were not numerical. Given that the audio vector was used to categorize songs in the mentioned neural network, the hypothesis was that clustering analysis could also be done exclusively on that feature-space. The dataset was therefore preprocessed to only include the subspace of audio-features.

As a next step in preprocessing, features were normalized to the same variance --- a way to make sure that all features have the same impact on the algorithm. For normalization \textit{Z-score} was used, see \cref{eq:z-score}. $X_j$ is the vector of all datapoints for feature $j$, $\mu_j$ is the mean values for datapoints in feature $j$, $\sigma_{j}$ is the standard deviation of feature $j$ and $Z_j$ is the normalized vector.

\begin{equation}
  \label{eq:z-score}
  Z_j = \frac{X_j - \mu_j}{\sigma_j}
\end{equation}

\section{Implementation}

\subsection{EWKM}
An implementation of the EWKM algorithm is available in \textit{R} and \textit{CSRAN} through the \textit{wskm} package \cite{wskm2014hz}. The code is written in $C$ and wrapped for $R$. All of the source code is obtainable from an online repository\footnote{https://github.com/SimonYansenZhao/wskm/}. The code has been extended by the author to work with \textit{Python} and Numpy\cite{numpy-c} --- by re-wrapping the code.

There are modifications made in the implementation that differ from the original research paper \cite{Jing2007}. Additional normalization steps have been added when calculating $w_{lj}$. \Cref{eq:newlambda} shows how $w_{lj}$, the feature weight, is updated in the given implementation. Occurrences of empty clusters during partitioning are managed by re-sampling cluster centers, i.e. restarting the algorithm; in the original algorithm empty clusters were not treated. Finally, \begin{color}{modified}convergence is defined to occur if the value difference between cost functions of two iterations is within a user-defined percentage\end{color}. If the difference is greater than the assigned percentage another iteration will occur.

\begin{align}
  \lambda_{lj} &= \exp\bigg({\Big(\frac{-D_{lj}}{\gamma}\Big) - \max_{\forall{t} \in C_l}\Big({\frac{-D_{lt}}{\gamma}}\Big)}\bigg) \\
  \lambda_{lj} &= \max\bigg(\frac{\lambda_{lj}}{\sum_{t=1}^{m}{\lambda_{lt}}}, \frac{0.0001}{m}\bigg)\\
  \label{eq:newlambda}
  w_{lj} &= \frac{\lambda_{lj}}{\sum^{m}_{t=1}{\lambda_{ lt }}}
\end{align}

% Per request of the stakeholder along with the author's familiarity with Python and Pandas DataFrame, the code was re-wrapped for Python using numpy-C-API \cite{numpy-c} --- allowing for the dataset to be sent in to the algorithm as a numpy array. The C-code was tweaked to not be dependent on R's \textit{unif\_rand() function}.

\subsection{FSC}
The FSC implementation was created by the thesis author and is based on the C-code of EWKM. The methods of calculating cost, updating weights and updating cluster memberships were modified to correspond to the FSC algorithm as shown in \cref{eq:cost-fsc}, and \cref{eq:fsc}. $\gamma$ is replaced by $\beta$, and the small value of $\epsilon$ was set to $0.001$ as proposed in \cite{Gan2006}.

\subsection{LEKM}
LEKM like FSC was created by the author and based on the source code of EWKM \cite{wskm2014hz}. The same steps are necessary for this algorithm as EWKM, since both are based on the negative entropy. The difference is the addition of logarithmic distances when updating the centers, partitions, and feature weights. How they were updated correspond to the equations of LEKM shown in \crefrange{eq:lekm}{eq:lekm2}.

A slight difference is that the right-hand side, i.e. the negative entropy was dropped when updating partitions. The modification is based on a discussion with one of the authors of the original \textit{LEKM} paper. The gist of the discussion was that the entropy should not be used for updating clusters, and could result in negative distances.

\subsection{k-means}
EWKM is equivalent to k-means when $\lim_{\gamma \to 0}$ although simply setting $\gamma = 0$ forces division by zero. For this reason k-means needs its own implementation.

k-means is available for Python thorough various packages such as PyClustering \cite{Novikov2019}. However, to allow a fair comparison between algorithms, k-means was re-implemented by the author in \textit{C} based on the EWKM source code.

% All algorithm specific parameters were ran with all $k$ candidates. The best performing combination of parameters are deemed the best candidates for the algorithm.
\section{Parameter Selection}
% The sections below describe how the parameters of the different algorithms were chosen.
Algorithms were tested with multiple candidate parameters on the \begin{color}{modified}{same complete sample of $5 \cdot 10^{4}$ songs}\end{color}, in order to find the most suitable parameters. The impact of the different parameters was compared through an external validation index, a \textit{purity} score that measured the genre purity of clusters. The best parameters in accordance to the index were deemed the most suitable, and chosen as the parameters for the algorithm for further tests. 

\begin{color}{modified}
\Cref{table:hyperparameters} summarizes the hyperparameters of the different algorithms. Two parameters $\phi$ --- the cost ratio between two iterations needed for convergence --- and $\epsilon$ --- the small constant to avoid division by zero for FSC --- were not tuned. Convergence was set to occur if the cost ratio was less than $0.5\%$ i.e. $\phi = 0.5$. Algorithm specific parameter selection is described in the sections below.
\begin{table}[H]
\makebox[\linewidth][c]{
\begin{color}{modified}
\begin{tabular}{cll}
  \hline\noalign{\smallskip}
  \multicolumn{1}{l}{\tbtitle{Hyperparameter}} & \multicolumn{1}{c}{\tbtitle{Description}} & \tbtitle{Algorithm}\\
\noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  $k$ & Number of clusters & All \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  $\phi^*$ & Ratio for convergence (constant)  & All \\
  $\epsilon^*$ & Small constant to avoid division by zero  & FSC \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  $\beta$ & Weighing factor for fuzzy-weighting algorithms  & FSC \\
  $\gamma$ & Entropy regularization factor & EWKM, LEKM \\
\end{tabular}
\end{color}
}
\caption{Hyperparameters of the given algorithms ($\phi$, $\epsilon$ were set as constants a priori)}
\label{table:hyperparameters}
\end{table}
\end{color}


\subsection{Purity}
\begin{color}{modified}
An external validation index was created using the \textit{genre} feature as a ground truth in combination with the measurement of purity. To create the purity score as shown in \cref{eq:purity}, the most dominant genre of each cluster --- found through \cref{eq:purity_truth}, where $G = \{g_1,g_2,...,g_p\}$ is the set of possible genres in the dataset, was aggregated and divided by the total amount of songs in the dataset.
\end{color}

\begin{equation}
  \Psi_{l} = \argmax_{1 \leq o \leq p} \sum_{X_i \in C_l}{\big[X_i (genre) = g_o\big]}
  \label{eq:purity_truth}
\end{equation}



\subsection{Amount of Clusters}
The problem of deciding the amount of clusters ($k$) is central for all the chosen clustering algorithms. This section describes how $k$ was chosen for all of the chosen algorithms.

The choice of $k$ was chosen to be based on the purity results of the clustered dataset. A sampling approach was declined as the dataset was already a manageable size. Competitive learning was also declined as the strategy would add another layer of complexity on the algorithms and a possible point of error.

% The amount of clusters i.e. $k$, could have been decided on a smaller sample of the dataset, or through the use of a competitive learning strategy as discussed in the background. Due to the already relative small sample of the whole dataset and an algorithm that is linear in time, $k$ was based on the results of the whole sample dataset with respect to the best $\gamma$ in regards to the cost function.

Candidate values of $k$ were restricted to $50$, $100$, and $500$ in order to reduce the time needed to generate results. The values were chosen to give an approximation on how large \textit{k} should be. The minimum amount of clusters ($50$) was chosen to be larger than the $33$ genres in the dataset. $500$ was decided as the maximum amount for $k$ to keep the average cluster size to be above $100$.

% For the later external evaluation stage with composers, $k$ was picked based on the overall best purity score of the algorithms.

% Relying on this criteria to evaluate the results had some downsides. A good song cluster does not have to be genre homogenic. A good cluster could hypothetically have been Christmas music and hold multiple genres. According to purity, this would be a bad cluster.

% The decision was still made to use purity for parameter tuning. While it does have its' downsides, the belief is that a higher purity score should in general indicate higher average cluster quality.

% To not disregard the problems with purity another external evaluation was made with the best performing SSC algorithm and k-means through the usage of judges. The evaluation is described in the next subchapter.

\subsection{EWKM}
EWKM introduces the $\gamma$ parameter. The recommended range of $\gamma \approx (0.5, 3)$ which was mentioned in \citeauthor{Jing2007} \cite{Jing2007} and \citeauthor{wskm2014hz} \cite{wskm2014hz}, was extended with smaller values $(0.0005,$ $0.001,$ $0.005,$ $0.01,$ $0.05,$ $0.1)$ as candidate values. The smaller values were added to avoid possible problems with the objective function being negative due to a large negative entropy.

Results of gamma values with immediate convergence were discarded as the behaviour was deemed undesirable, and not what was expected from the algorithms.

\subsection{FSC}
The unique parameters of \textit{FSC}, $\beta$ --- the fuzziness variable, and $\epsilon$ --- a small constant, were set to $2.1$ and $0.00001$, respectively, in \cite{Gan2006}. In our tests $\beta$ was varied between $1.5$ and $30$ while $\epsilon$ was kept to the value mentioned in the report \citeauthor{Gan2006} \cite{Gan2006}.

\subsection{LEKM}
In \citeauthor{Gan2016} \cite{Gan2016} values of $\beta$ were varied between $1$ and $16$ with a decreasing score after a value of $2.0$. Our candidate values ranged between $0.5$ and $10$. Similarly to EWKM, results of immediate convergence were disregarded.

% \begin{color}{red}
% $\gamma$ is a hyper-parameter of EWKM and LEKM. In short, $\gamma$ determines the likeliness to cluster on more or less features. For any given dataset the ideal $\gamma$ can vary. An ideal $\gamma$ is found (in this scenario) when there is no other $\gamma$ value that can result in a better purity.
% \end{color}

% $\gamma$ was chosen by iteratively testing different values of the parameter as an input for the algorithms, from small ($1 * 10^{-3}$) to large ($\gamma = 3$). The best $\gamma$ was chosen based on purity and, if values showed similar results the smallest value was chosen.

\subsection{k-means}
k-means does not have any algorithm specific hyper-parameter that needs to be tweaked. The only thing that was necessary for the result generation was to run k-means on all values of $k$.

\section{Ranking songs and clusters}
Songs were sorted in ascending order based on their distance to their \begin{color}{modified}respective cluster center. The distance is equivalent to the distance used for assigning a partition to an object as shown in \cref{ch:back}. For k-means that was the regular Euclidean distance. For SSC algorithms the distance included the feature weights of the cluster. \end{color}

Clusters were similarly sorted in ascending order. The distance for a cluster was set as the average distance of songs with a membership in the cluster i.e. the average distance to the cluster center for songs partitioned in the cluster.
\begin{color}{modified}
The sorting was used as a ranking, based on the idea that "better" clusters would have smaller intra-cluster distances than "worse" clusters. Genre accuracy was not used for the ranking in order to allow clusters that have novel themes --- that are not genre-specific --- a chance to be ranked high. From an application perspective it is more valuable to find non-genre specific themes, as genre specific themes are easily found through genre filtering. Another option would be inter-cluster distances, but deciding on a inter-cluster distance is non-trivial and algorithm dependent.
\end{color}

The ranking allowed for a cutoff in songs and clusters, i.e. the top five clusters could be chosen. To hinder the possibility of the same genre occurring in multiple clusters, each genre could only appear as dominant in one cluster (except for pop and rock, as they were regarded as larger genres with more diversity). As such after sorting, the top five clusters were picked from small to large distance with the condition stated above.

\section{Expert Evaluation}
To answer how performance in terms of novelty and cohesion (similarity) varied between traditional and SSC algorithms, a blind test was created \begin{color}{modified} where professional composers were used as evaluation judges.

Initially, the idea was that composers would rate a cluster on \textit{similarity} and \textit{novelty} (from a scale of 1-10). After showing a mock test and discussing it with the head of product experience, head of machine learning, and a number of composers at the stakeholder, it was conducted that the parameters had to be modified. There were two reasons: the terms were unclear for the composers, and a general quality was hard to transcribe from the scores.

To adhere to the problems, \textit{similarity} was divided into \textit{audio similarity} and \textit{cultural similarity}. \textit{Novelty} was changed to a term more familiar to the composers --- \textit{playlist uniqueness}, i.e. how unique the cluster would be as a playlist compared to already existing playlists on the platform. Additionally, a \textit{general quality} criteria was added to solve the second problem of scores not translating to a general quality. Evaluators were also allowed to add any additional description they thought summarized the cluster. The final criteria are shown in \cref{table:external_desc}.

\medskip
\begin{table}[H]
\begin{color}{modified}
\makebox[\linewidth][c]{
\begin{tabular}{ll}
  \hline\noalign{\smallskip}
  \multicolumn{1}{l}{\tbtitle{Criteria}} & \multicolumn{1}{c}{\tbtitle{Description}} \\
\noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  Audio Similarity & How well do songs in the cluster share audio patterns? \\
  Cultural Similarity & How well do songs in the cluster share an audience? \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  Playlist Uniqueness & How easy would it be for a composer come up with a similar playlist? \\
  \noalign{\smallskip}
  General Quality & How useful is the cluster for playlist creation?  \\
  \noalign{\smallskip}
  \hline
\end{tabular}
}
\caption{Description of criteria for external evaluation, each criteria was rated by the composers with an integer scale of 1 to 10}
\label{table:external_desc}
\end{color}
\end{table}

\begin{color}{modified}
The platform of the evaluation was a webpage created by the author. In here, 15 different clusters could be browsed --- 5 of each source (k-means, SSC-algorithm and existing playlists). Each cluster was represented by 10 30-second song previews. The chosen songs were sampled based on a half-normal distribution, i.e. \begin{color}{modified}songs with a relative small distance to the cluster center \end{color} had a higher chance to be picked than those with a high distance. A picture of the webpage is shown in \cref{fig:website}. Django \footnote{https://www.djangoproject.com/} and Bootstrap 4\footnote{https://getbootstrap.com/} were used to build the page.

The group of evaluators consisted of six composers. The chosen composers work professionally to create playlists that fit the needs of different retailers. They were deemed suitable as judges due to their experience in playlist creation. The composers would also be the beneficiaries of a clustering solution.\end{color}

The test was conducted as a blind test to remove any bias composers might have. The composers were not told that clusters had different sources. Instead they were told that all clusters were from the same machine learning algorithm. \begin{color}{modified}To avoid possible suspicion, the presentation order of clusters were randomized. Information shown of clusters were also consistent between sources.\end{color}

At the time of the evaluation, the created website was displayed onto a TV in a conference room. All 6 composers were situated in the room together with the conductor of the blind test. The test started by composers filling out their name, after which a random cluster was displayed on the screen. When a cluster was displayed, composers started their evaluation by listening to all the 10 songs shown, with varying durations --- some songs did not require 30 seconds to understand how the song affected the cluster in regards to the chosen criteria. The next step involved a pre-criteria grading discussion with all composers, this discussion was used to conclude the groups general thoughts of the cluster. Finally, the cluster was discussed on each specific criteria and then graded from 1 to 10 through a slider. After the submission of a graded cluster, a new cluster was displayed. Throughout the test, discussions composers had were recorded by the conductor. Additionally, questions composers had on webpage navigation and general criteria considerations were answered by the conductor. In total the time for the evaluation took 2 hours and involved 5 clusters of each cluster source.


To evaluate the scores given by the composers on the three different cluster sources, a null hypothesis and an alternate hypothesis were created as shown in \cref{eq:null} and \cref{eq:alternate}.
A single-factor ANOVA test ($\alpha = 0.05$) was used to test the null hypothesis. Each criteria were subjected to the test in order to understand if there was any significant difference between the means of the given sources.

\begin{align}
  \label{eq:null}
  H_0 &= \bigg[ \mu_{\textit{pl}} = \mu_{\textit{SSC}} = \mu_{\textit{k-means}} \bigg] \\
  \label{eq:alternate}
  H_a &= \bigg[ \mu_{\textit{pl}} \neq \mu_{\textit{SSC}} \neq \mu_{\textit{k-means}} \bigg]
\end{align}

\end{color}

\begin{figure}[h!]
  \centering
\centerline{\includegraphics[trim={10pt 10pt 20pt, 10pt},clip, scale=1.0, keepaspectratio]{Website}}
    \caption{Screenshot of the evaluation website. \newline
      From the left-hand-side song samples are shown and played. In the middle, statistics about genres and common artists are shown. On the right-hand-side, the evaluation form is situated with a general description text area, and sliders for the rest of the evaluation categories.
  }\label{fig:website}
\end{figure}


\end{document}

