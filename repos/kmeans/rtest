#!/usr/bin/python3.7
import sys
import os

import click
import numpy as np
import pandas as pd
from EWKM import EWKM

import clustering_utils as cs


# argv

@click.command()
@click.option('-p', "--re-parse", is_flag=True, default=False, help="Reparse clustering_data.json")
@click.option('-k', default=50, help="Amount of clusters")
@click.option('-l', "--lamb", default=1.0, help="Lambda, 0<l<inf, likeliness to represent clusters with more attributes")
@click.option('-i', "--max-iter", default=100, help="Maximum number of iterations")
@click.option('-d', "--convergance-delta", default=0.05, help="convergance delta")
@click.option('-r', "--max-restart", default=0, help="Allow restarts value determines amount of max. restarts")
@click.option('-m', "--mult-lamb", default="", help="Send in multiple lamb values")
@click.option("--mult-k", default="", help="Send in multiple k values")
@click.option('-n', "--name", default="", help="name of test")
@click.option('-u', "--k-plusplus/--random-uniform", default=False, help="Only use audio embedding ignore other numerical data")
def main(re_parse, k, lamb, max_iter, convergance_delta, max_restart, mult_lamb, mult_k, name, k_plusplus):
    lambdas = np.fromstring(mult_lamb, dtype=float, sep=' ')
    path = "clusters/{0}/{1}".format("ewkmtest", name)

    data = pd.read_csv('data/ewkmtest.csv', sep=',', header=0)
    print(data)
    numerical_data = data.copy()
    # numerical_data = data.agg(cs.min_max_scaling, axis=0)



    if lambdas.size is not 0:
        best_lamb = -1
        min_disp = float("inf")

        os.mkdir(path)
        costs = []
        scores = []
        restart_list = []
        purities = []

        for i in range(0, lambdas.size):
            dispersion, clusters, iterations, \
                restarts, tot_iter, silhouette_score, purity = run_for_lambda(
                    mixed_data=data,
                    numerical_data=numerical_data,
                    k=k,
                    lamb=lambdas[i],
                    max_iter=max_iter,
                    delta=convergance_delta,
                    max_restart=max_restart,
                    path=path,
                    k_plusplus=k_plusplus,
                )

            print(f'dispersion: {dispersion}, restarts: {restarts}, iterations: {iterations}, tot_iter: {tot_iter}')
            # print(f'cal: {calinski_harabaz_score(numerical_data, clusters)}')
            # print(f'sil: {silhouette_score(numerical_data, clusters)}')
            costs.append(dispersion)
            scores.append(silhouette_score)
            restart_list.append(restarts)
            purities.append(purity)

            if dispersion < min_disp and iterations > 0:
                min_disp = dispersion
                best_lamb = lambdas[i]

        print(f'best dispersion: {min_disp}, lambda: {best_lamb}')
        cs.generate_run_specific_json(lambdas, costs, scores, purities, path)
        cs.generate_run_specific_plot(
            lambdas,
            costs,
            scores,
            purities,
            iterations,
            restart_list,
            path
        )

    else:
        dispersion, clusters, iterations, \
            restarts, tot_iter, silhouette_score, purity = run_for_lambda(
                mixed_data=data,
                numerical_data=numerical_data,
                k=k,
                lamb=lamb,
                max_iter=max_iter,
                delta=convergance_delta,
                max_restart=max_restart,
                path=path,
                k_plusplus=k_plusplus
            )

        print(f'dispersion: {dispersion}')


def run_for_lambda(
        mixed_data,
        numerical_data,
        k,
        lamb,
        max_iter,
        delta,
        max_restart,
        path,
        k_plusplus
):
    ewkm = EWKM(numerical_data)
    dispersion, clusters, centers, \
        weights, iterations, restarts, tot_iter, distances = ewkm.predict(
            k=k,
            lamb=lamb,
            max_iter=max_iter,
            delta=delta,
            max_restart=max_restart,
            init=int(k_plusplus)
        )

    # silhouette_score = ewkm.silhouette()
    # print('sil: {0}'.format(silhouette_score))

    merged_data = cs.merge_columns(mixed_data, clusters, distances)

    str_lamb = str(lamb).replace('.', '-')
    parameter_path = "{0}/K_{1}_L_{2}".format(path, k, str_lamb)
    os.mkdir(parameter_path)
    # maybe let it also add some statistics
    generate_cluster_specific_json(
        merged_data, k, lamb, weights, parameter_path)
    cs.generate_parameter_specific_plots(weights, k, lamb, parameter_path)
    # purity = cs.get_purity(merged_data)
    # silhouette_score = sk.silhouette()
    # print('sil: {0}'.format(silhouette_score))

    return dispersion, clusters, iterations, \
        restarts, tot_iter, 0, 0


def nested_dataframe(data, weights):
    """Generate a nested dataframe
    """

    # group by cluster
    # No need to keep the embeddings or cluster tag per song
    to_keep_filter = list(data.filter(
        regex="^(?!(embeddings|cluster|version|fully_tagged))"))
    grouped = data.groupby(['cluster'], as_index=False)
    grouped_mean = grouped.mean()
    grouped_size = grouped.size()
    grouped_size = grouped_size.rename("count")

    # create a nested dataframe
    j = (grouped.apply(lambda x: x[to_keep_filter].to_dict('r'))
         .reset_index()
         .rename(columns={'index': 'cluster'})
         .merge(grouped_mean, left_on='cluster', right_on='cluster', how='inner')
         .merge(grouped_size, left_on='cluster', right_on='cluster', how='inner')
         .rename(columns={0: 'songs'})
         )

    if type(weights) is np.ndarray:
        try:
            print(weights.shape)

            rows, columns = weights.shape

            # Only show the highest weights
            w_list = weights.tolist()
            w = [
                dict(
                    sorted(
                        zip(range(0, columns), t),
                        key=lambda x: x[1],
                        reverse=True
                    )[:10]
                )
                for t in w_list
            ]
            # somehow have to handle that empty clusters are already removed in j
            weight_frame = pd.DataFrame({'cluster': list(range(0, len(w))), 'hi_w': w})
            j = j.merge(weight_frame, left_on='cluster', right_on='cluster', how='inner')
            return j
        except:
            print(f'possible empty cluster problem')
            print(f'Dataframe shape {j.shape}')
            pass

def generate_cluster_specific_json(data, k, lamb, weights=None, path=None):
    """Generate a nested summary json
    And a json for each cluster containing the songs
    sorted by the average distance to cluster center
    """
    j = data

    if(type(weights) is np.ndarray):
        j = nested_dataframe(j, weights)

    j = j.query('count > 1').sort_values(by='distance')

    # reorder the columns
    cols = ["cluster", "distance", "count"] + \
        list(j.filter(regex="^(?!(cluster|distance|count|songs))")) + ["songs"]
    j = j[cols]

    # summary json
    j_summary = j.drop(columns=['songs'])

    j_summary.to_json(
        path + '/summary.json',
        orient='records'
        # lines=True
    )

    # json for each specific cluster
    for i in range(0, k):
        try:
            j[j['cluster'] == i].to_json(
                "{0}/{1}.json".format(path, i),
                orient='records',
                lines=True
            )
        except:
            print(f'skipped writing {i}.json due to no members')
            continue
main()
