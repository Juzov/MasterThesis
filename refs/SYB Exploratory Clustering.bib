Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{diday1976clustering,
author = {Diday, Edwin and Simon, J C},
booktitle = {Digital pattern recognition},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/oldestbook.pdf:pdf},
pages = {47--94},
publisher = {Springer},
title = {{Clustering analysis}},
year = {1976}
}
@article{Parsons2004,
address = {New York, NY, USA},
author = {Parsons, Lance and Haque, Ehtesham and Liu, Huan},
doi = {10.1145/1007730.1007731},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/reviewsubspace.pdf:pdf},
issn = {1931-0145},
journal = {SIGKDD Explor. Newsl.},
keywords = {clustering survey,high dimensional data,projected clustering,subspace clustering},
month = {jun},
number = {1},
pages = {90--105},
publisher = {ACM},
title = {{Subspace Clustering for High Dimensional Data: A Review}},
url = {http://doi.acm.org/10.1145/1007730.1007731},
volume = {6},
year = {2004}
}
@inproceedings{Li2008,
abstract = {Traditional k-means algorithm can make the distances of objects in the same cluster as small as possible, but the distances of objects from different clusters are not satisfied efficiently and usually the dataset with mixed numeric and categorical data is not classified correctly. The IWEKM (improved weight entropy k-means) algorithm is proposed in this paper. The proposed algorithm overcomes the above problems by modifying the cost function of entropy weighting k-means clustering algorithm by adding a variable that is relevant linearly to the square sum of distances from the mean of all objects and the means of all clusters and a variable that is relevant to relativity degree of categorical data. The results of different clustering algorithms applied on Iris data and Flag data show that the proposed algorithm is efficient.},
author = {Li, T and Chen, Y},
booktitle = {2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery},
doi = {10.1109/FSKD.2008.32},
keywords = {Clustering algorithms,Conference management,Cost function,Entropy,Flag data,Fuzzy systems,IWEKM,Iris,Iris data,Knowledge management,Partitioning algorithms,Utility programs,categorical data,clustering,cost function,entropy,k-means algorithm,numeric data,partition clustering,pattern clustering,weight entropy,weight entropy k-means algorithm},
month = {oct},
pages = {36--41},
title = {{A Weight Entropy k-Means Algorithm for Clustering Dataset with Mixed Numeric and Categorical Data}},
volume = {1},
year = {2008}
}
@article{Ng1999,
abstract = {This correspondence describes extensions to the fuzzy k-means algorithm for clustering categorical data. By using a simple matching dissimilarity measure for categorical objects and modes instead of means for clusters, a new approach is developed, which allows the use of the k-means paradigm to efficiently cluster large categorical data sets. A fuzzy k-modes algorithm is presented and the effectiveness of the algorithm is demonstrated with experimental results.},
author = {Ng, M K},
doi = {10.1109/91.784206},
issn = {1063-6706},
journal = {IEEE Transactions on Fuzzy Systems},
keywords = {Australia,Clustering algorithms,Clustering methods,Computational complexity,Computational efficiency,Data mining,Databases,Information management,Partitioning algorithms,categorical data clustering,data mining,fuzzy k-modes algorithm,fuzzy set theory,matching dissimilarity measure,pattern clustering},
month = {aug},
number = {4},
pages = {446--452},
title = {{A fuzzy k-modes algorithm for clustering categorical data}},
volume = {7},
year = {1999}
}
@misc{wskm,
author = {{Graham Williams, Joshua Z Huang, Xiaojun Chen}, Qiang Wang and Longfei Xiao},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{wskm: Weighted k-Means Clustering. R package version 1.4.28}},
url = {https://cran.r-project.org/web/packages/wskm/index.html},
year = {2015}
}
@article{van2009,
author = {{Van Der Maaten}, Laurens and Postma, Eric and den Herik, Jaap},
title = {{Dimensionality reduction: a comparative}},
year = {2009}
}
@inbook{Jolliffe2005,
abstract = {Abstract When large multivariate datasets are analyzed, it is often desirable to reduce their dimensionality. Principal component analysis is one technique for doing this. It replaces the p original variables by a smaller number, q, of derived variables, the principal components, which are linear combinations of the original variables. Often, it is possible to retain most of the variability in the original variables with q very much smaller than p. Despite its apparent simplicity, principal component analysis has a number of subtleties, and it has many uses and extensions. A number of choices associated with the technique are briefly discussed, namely, covariance or correlation, how many components, and different normalization constraints, as well as confusion with factor analysis. Various uses and extensions are outlined.},
author = {Jolliffe, Ian},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
doi = {10.1002/0470013192.bsa501},
isbn = {9780470013199},
keywords = {dimension reduction,factor analysis,multivariate analysis,variance maximization},
publisher = {American Cancer Society},
title = {{Principal Component Analysis}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470013192.bsa501},
year = {2005}
}
@book{zhou2012ensemble,
author = {Zhou, Zhi-Hua},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/ensemble.pdf:pdf},
isbn = {9781439830031},
pages = {135--156},
publisher = {Chapman and Hall/CRC},
title = {{Ensemble methods: foundations and algorithms}},
year = {2012}
}
@article{Domeniconi2007,
abstract = {Clustering suffers from the curse of dimensionality, and similarity functions that use all input features with equal relevance may not be effective. We introduce an algorithm that discovers clusters in subspaces spanned by different combinations of dimensions via local weightings of features. This approach avoids the risk of loss of information encountered in global dimensionality reduction techniques, and does not assume any data distribution model. Our method associates to each cluster a weight vector, whose values capture the relevance of features within the corresponding cluster. We experimentally demonstrate the gain in perfomance our method achieves with respect to competitive methods, using both synthetic and real datasets. In particular, our results show the feasibility of the proposed technique to perform simultaneous clustering of genes and conditions in gene expression data, and clustering of very high-dimensional data such as text data.},
author = {Domeniconi, Carlotta and Gunopulos, Dimitrios and Ma, Sheng and Yan, Bojun and Al-Razgan, Muna and Papadopoulos, Dimitris},
doi = {10.1007/s10618-006-0060-8},
issn = {1573-756X},
journal = {Data Mining and Knowledge Discovery},
month = {feb},
number = {1},
pages = {63--97},
title = {{Locally adaptive metrics for clustering high dimensional data}},
url = {https://doi.org/10.1007/s10618-006-0060-8},
volume = {14},
year = {2007}
}
@article{Deng2010,
abstract = {While within-cluster information is commonly utilized in most soft subspace clustering approaches in order to develop the algorithms, other important information such as between-cluster information is seldom considered for soft subspace clustering. In this study, a novel clustering technique called enhanced soft subspace clustering (ESSC) is proposed by employing both within-cluster and between-class information. First, a new optimization objective function is developed by integrating the within-class compactness and the between-cluster separation in the subspace. Based on this objective function, the corresponding update rules for clustering are then derived, followed by the development of the novel ESSC algorithm. The properties of this algorithm are investigated and the performance is evaluated experimentally using real and synthetic datasets, including synthetic high dimensional datasets, UCI benchmarking datasets, high dimensional cancer gene expression datasets and texture image datasets. The experimental studies demonstrate that the accuracy of the proposed ESSC algorithm outperforms most existing state-of-the-art soft subspace clustering algorithms.},
author = {Deng, Zhaohong and Choi, Kup-Sze and Chung, Fu-Lai and Wang, Shitong},
doi = {https://doi.org/10.1016/j.patcog.2009.09.010},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/essc.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {-insensitive distance,Gene expression clustering analysis,Soft subspace,Subspace clustering,Texture image segmentation,Weighted clustering},
number = {3},
pages = {767--781},
title = {{Enhanced soft subspace clustering integrating within-cluster and between-cluster information}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320309003483},
volume = {43},
year = {2010}
}
@inproceedings{Huang97clusteringlarge,
author = {Huang, Zhexue},
booktitle = {In The First Pacific-Asia Conference on Knowledge Discovery and Data Mining},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/kproto.pdf:pdf},
pages = {21--34},
title = {{Clustering large data sets with mixed numeric and categorical values}},
year = {1997}
}
@article{Jia2018,
abstract = {In clustering analysis, data attributes may have different contributions to the detection of various clusters. To solve this problem, the subspace clustering technique has been developed, which aims at grouping the data objects into clusters based on the subsets of attributes rather than the entire data space. However, the most existing subspace clustering methods are only applicable to either numerical or categorical data, but not both. This paper, therefore, studies the soft subspace clustering of data with both of the numerical and categorical attributes (also simply called mixed data for short). Specifically, an attribute-weighted clustering model based on the definition of object-cluster similarity is presented. Accordingly, a unified weighting scheme for the numerical and categorical attributes is proposed, which quantifies the attribute-to-cluster contribution by taking into account both of intercluster difference and intracluster similarity. Moreover, a rival penalized competitive learning mechanism is further introduced into the proposed soft subspace clustering algorithm so that the subspace cluster structure as well as the most appropriate number of clusters can be learned simultaneously in a single learning paradigm. In addition, an initialization-oriented method is also presented, which can effectively improve the stability and accuracy of k-means-type clustering methods on numerical, categorical, and mixed data. The experimental results on different benchmark data sets show the efficacy of the proposed approach.},
author = {Jia, Hong and Cheung, Yiu Ming},
doi = {10.1109/TNNLS.2017.2728138},
file = {:home/ejuzovitski/Downloads/08000671.pdf:pdf},
isbn = {2016053119},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Attribute weight,categorical-and-numerical data,initialization method,number of clusters,soft subspace clustering},
number = {8},
pages = {3308--3325},
pmid = {28792907},
publisher = {IEEE},
title = {{Subspace clustering of categorical and numerical data with an unknown number of clusters}},
volume = {29},
year = {2018}
}
@misc{numpy-c,
title = {{⁷NumPy C-API — NumPy Manual}},
url = {https://docs.scipy.org/doc/numpy/reference/c-api.html}
}
@article{Wunsch2005,
abstract = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
author = {Wunsch, D},
doi = {10.1109/TNN.2005.845141},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Adaptive resonance theory (ART),Algorithms,Application software,Automated,Bioinformatics,Clustering algorithms,Computer Simulation,Computer science,Computer-Assisted,Data analysis,Humans,Machine learning,Machine learning algorithms,Models,Neural Networks (Computer),Numerical Analysis,Pattern Recognition,Signal Processing,Statistical,Statistics,Stochastic Processes,Traveling salesman problems,benchmark data sets,bioinformatics,cluster analysis,cluster validation,clustering,clustering algorithm,data analysis,neural networks,pattern classification,pattern clustering,proximity,self-organizing feature map (SOFM),traveling salesman problem},
month = {may},
number = {3},
pages = {645--678},
title = {{Survey of clustering algorithms}},
volume = {16},
year = {2005}
}
@article{CHEN2016271,
abstract = {Most data streams encountered in real life are data objects with mixed numerical and categorical attributes. Currently most data stream algorithms have shortcomings including low clustering quality, difficulties in determining cluster centers, poor ability for dealing with outliers' issue. A fast density-based data stream clustering algorithm with cluster centers automatically determined in the initialization stage is proposed. Based on data attribute relationships analysis, mixed data sets are filed into three types whose corresponding distance measure metrics are designed. Based on field intensity-distance distribution graph for each data object, linear regression model and residuals analysis are used to find the outliers of the graph, enabling cluster centers automatic determination. After the cluster centers are found, all data objects can be clustered according to their distance with centers. The data stream clustering algorithm adopts an online/offline two-stage processing framework, and a new micro cluster characteristic vector to maintain the arriving data objects dynamically. Micro clusters decay function and deletion mechanism of micro clusters are used to maintain the micro clusters, which reflects the data stream evolution process accurately. Finally, the performances of the proposed algorithm are testified by a series of experiments on real-world mixed data sets in comparison with several outstanding clustering algorithms in terms of the clustering purity, efficiency and time complexity.},
author = {Chen, Jin-Yin and He, Hui-Hao},
doi = {https://doi.org/10.1016/j.ins.2016.01.071},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, He - 2016 - A fast density-based data stream clustering algorithm with cluster centers self-determined for mixed data.pdf:pdf},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {Data mining,Data stream clustering,Mixed attributes,Mixed distance measure metrics,Peak field intensity},
pages = {271--293},
title = {{A fast density-based data stream clustering algorithm with cluster centers self-determined for mixed data}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025516300032},
volume = {345},
year = {2016}
}
@article{DBLP:journals/corr/abs-cs-0509011,
archivePrefix = {arXiv},
arxivId = {cs/0509011},
author = {He, Zengyou and Xu, Xiaofei and Deng, Shengchun},
eprint = {0509011},
journal = {CoRR},
primaryClass = {cs},
title = {{Clustering Mixed Numeric and Categorical Data: {\{}A{\}} Cluster Ensemble Approach}},
url = {http://arxiv.org/abs/cs/0509011},
volume = {abs/cs/050},
year = {2005}
}
@article{Ng2002,
abstract = {Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. To this end, this paper has three main contributions. First, we propose a new clustering method called CLARANS, whose aim is to identify spatial structures that may be present in the data. Experimental results indicate that, when compared with existing clustering methods, CLARANS is very efficient and effective. Second, we investigate how CLARANS can handle not only points objects, but also polygon objects efficiently. One of the methods considered, called the IR-approximation, is very efficient in clustering convex and nonconvex polygon objects. Third, building on top of CLARANS, we develop two spatial data mining algorithms that aim to discover relationships between spatial and nonspatial attributes. Both algorithms can discover knowledge that is difficult to find with existing spatial data mining algorithms. Index Terms-Spatial data mining, clustering algorithms, randomized search, computational geometry.},
author = {Ng, Raymond T and Han, Jiawei},
doi = {10.1109/TKDE.2002.1033770},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Han - Unknown - CLARANS A Method for Clustering Objects for Spatial Data Mining.pdf:pdf},
journal = {IEEE Transaction on Knowledge and Data Engineering},
title = {{CLARANS : A method for clustering objects for}},
year = {2002}
}
@manual{wskm2014hz,
annote = {R package version 1.4.28},
author = {Williams, Graham and Huang, Joshua Zhexue and Chen, Xiaojun and Wang, Qiang and Xiao, Longfei},
title = {{wskm: Weighted k-Means Clustering}},
url = {http://cran.r-project.org/package=wskm},
year = {2015}
}
@inproceedings{Shirkhorshidi2014,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.1881v2},
author = {Shirkhorshidi, Ali Seyed and Aghabozorgi, Saeed and Wah, Teh Ying and Herawan, Tutut},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-09156-3_49},
eprint = {arXiv:1101.1881v2},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shirkhorshidi et al. - 2014 - Big Data Clustering A Review(2).pdf:pdf},
isbn = {9783319091556},
issn = {16113349},
keywords = {Big Data,Clustering,MapReduce,Parallel Clustering},
number = {PART 5},
pages = {707--720},
pmid = {17707831},
publisher = {Springer, Cham},
title = {{Big data clustering: A review}},
url = {http://link.springer.com/10.1007/978-3-319-09156-3{\_}49},
volume = {8583 LNCS},
year = {2014}
}
@inbook{Kaufman1990,
abstract = {Summary The prelims comprise: Motivation Types of Data and How to Handle Them Which Clustering Algorithm to Choose A Schematic Overview of Our Programs Computing Dissimilarities with the Program DAISY},
author = {Kaufman, Leonard. and Rousseeuw, Peter J.},
booktitle = {Finding Groups in Data},
chapter = {1},
doi = {10.1002/9780470316801.ch1},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufman, Rousseeuw - 2008 - Introduction.pdf:pdf},
isbn = {9780470316801},
keywords = {archeological findings,cluster analysis,interval-scaled variables,social sciences,spherical clusters},
pages = {1--67},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Introduction}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470316801.ch1},
year = {1990}
}
@article{huang2005automated,
author = {Huang, Joshua Zhexue and Ng, Michael K and Rong, Hongqiang and Li, Zichen},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/w-k-means.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis {\&} Machine Intelligence},
number = {5},
pages = {657--668},
publisher = {IEEE},
title = {{Automated variable weighting in k-means type clustering}},
year = {2005}
}
@article{Guha2000,
abstract = {Clustering, in data mining, is useful to discover distribution patterns in the underlying data. Clustering algorithms usually employ a distance metric based (e.g., euclidean) similarity measure in order to partition the database such that data points in the same partition are more similar than points in different partitions. In this paper, we study clustering algorithms for data with boolean and categorical attributes. We show that traditional clustering algorithms that use distances between points for clustering are not appropriate for boolean and categorical attributes. Instead, we propose a novel concept of links to measure the similarity/proximity between a pair of data points. We develop a robust hierarchical clustering algorithm ROCK that employs links and not distances when merging clusters. Our methods naturally extend to non-metric similarity measures that are relevant in situations where a domain expert/similarity table is the only source of knowledge. In addition to presenting detailed complexity results for ROCK, we also conduct an experimental study with real-life as well as synthetic data sets to demonstrate the effectiveness of our techniques. For data with categorical attributes, our findings indicate that ROCK not only generates better quality clusters than traditional algorithms, but it also exhibits good scalability properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Guha, Sudipto and Rastogi, Rajeev and Shim, Kyuseok},
doi = {10.1016/S0306-4379(00)00022-3},
eprint = {arXiv:1011.1669v3},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guha, Rastogi, Shim - 2000 - Rock a robust clustering algorithm for categorical attributes.pdf:pdf},
isbn = {0-7695-0071-4},
issn = {03064379},
journal = {Information Systems},
pmid = {25246403},
title = {{Rock: a robust clustering algorithm for categorical attributes}},
year = {2000}
}
@article{Halkidi2002,
abstract = {Clustering is an unsupervised process since there are no predefined classes and no examples that would indicate grouping properties in the data set. The majority of the clustering algorithms behave differently depending on the features of the data set and the initial assumptions for defining groups. Therefore, in most applications the resulting clustering scheme requires some sort of evaluation as regards its validity. Evaluating and assessing the results of a clustering algorithm is the main subject of cluster validity. In this paper we present a review of the clustering validity and methods. More specifically, Part I of the paper discusses the cluster validity approaches based on external and internal criteria.},
author = {Halkidi, Maria and Batistakis, Yannis and Vazirgiannis, Michalis},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/cval.pdf:pdf},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
number = {2},
pages = {40--45},
title = {{Cluster validity methods: part I}},
url = {http://portal.acm.org/citation.cfm?id=565117.565124},
volume = {31},
year = {2002}
}
@incollection{Shultz2011,
address = {Boston, MA},
author = {Shultz, Thomas R. and Fahlman, Scott E. and Craw, Susan and Andritsos, Periklis and Tsaparas, Panayiotis and Silva, Ricardo and Drummond, Chris and Ling, Charles X. and Sheng, Victor S. and Drummond, Chris and Lanzi, Pier Luca and Gama, Jo{\~{a}}o and Wiegand, R. Paul and Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and He, Jun and Jain, Sanjay and Stephan, Frank and Jain, Sanjay and Stephan, Frank and Sammut, Claude and Harries, Michael and Sammut, Claude and Ting, Kai Ming and Pfahringer, Bernhard and Case, John and Jain, Sanjay and Wagstaff, Kiri L. and Nijssen, Siegfried and Wirth, Anthony and Ling, Charles X. and Sheng, Victor S. and Zhang, Xinhua and Sammut, Claude and Cancedda, Nicola and Renders, Jean-Michel and Michelucci, Pietro and Oblinger, Daniel and Keogh, Eamonn and Mueen, Abdullah},
booktitle = {Encyclopedia of Machine Learning},
doi = {10.1007/978-0-387-30164-8_163},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Constrained Clustering.pdf:pdf},
pages = {220--221},
publisher = {Springer US},
title = {{Constrained Clustering}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-30164-8{\_}163},
year = {2011}
}
@article{Jing2007,
abstract = {This paper presents a new k-means type algorithm for clustering high-dimensional objects in sub-spaces. In high-dimensional data, clusters of objects often exist in subspaces rather than in the entire space. For example, in text clustering, clusters of documents of different topics are categorized by different subsets of terms or keywords. The keywords for one cluster may not occur in the documents of other clusters. This is a data sparsity problem faced in clustering high-dimensional data. In the new algorithm, we extend the k-means clustering process to calculate a weight for each dimension in each cluster and use the weight values to identify the subsets of important dimensions that categorize different clusters. This is achieved by including the weight entropy in the objective function that is minimized in the k-means clustering process. An additional step is added to the k-means clustering process to automatically compute the weights of all dimensions in each cluster. The experiments on both synthetic and real data have shown that the new algorithm can generate better clustering results than other subspace clustering algorithms. The new algorithm is also scalable to large data sets.},
author = {Jing, L and Ng, M K and Huang, J Z},
doi = {10.1109/TKDE.2007.1048},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/entropy{\_}subspace.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {data handling,data sparsity pro,pattern clustering},
month = {aug},
number = {8},
pages = {1026--1041},
title = {{An Entropy Weighting k-Means Algorithm for Subspace Clustering of High-Dimensional Sparse Data}},
volume = {19},
year = {2007}
}
@article{Cheung2013,
abstract = {Most of the existing clustering approaches are applicable to purely numerical or categorical data only, but not the both. In general, it is a nontrivial task to perform clustering on mixed data composed of numerical and categorical attributes because there exists an awkward gap between the similarity metrics for categorical and numerical data. This paper therefore presents a general clustering framework based on the concept of object-cluster similarity and gives a unified similarity metric which can be simply applied to the data with categorical, numerical, and mixed attributes. Accordingly, an iterative clustering algorithm is developed, whose outstanding performance is experimentally demonstrated on different benchmark data sets. Moreover, to circumvent the difficult selection problem of cluster number, we further develop a penalized competitive learning algorithm within the proposed clustering framework. The embedded competition and penalization mechanisms enable this improved algorithm to determine the number of clusters automatically by gradually eliminating the redundant clusters. The experimental results show the efficacy of the proposed approach.},
author = {Cheung, Yiu-ming and Jia, Hong},
doi = {https://doi.org/10.1016/j.patcog.2013.01.027},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/catnummix.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Categorical attribute,Clustering,Number of clusters,Numerical attribute,Similarity metric},
number = {8},
pages = {2228--2238},
title = {{Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320313000666},
volume = {46},
year = {2013}
}
@article{Chan2004,
abstract = {One of the main problems in cluster analysis is the weighting of attributes so as to discover structures that may be present. By using weighted dissimilarity measures for objects, a new approach is developed, which allows the use of the k-means-type paradigm to efficiently cluster large data sets. The optimization algorithm is presented and the effectiveness of the algorithm is demonstrated with both synthetic and real data sets.},
author = {Chan, Elaine Y and Ching, Wai Ki and Ng, Michael K and Huang, Joshua Z},
doi = {https://doi.org/10.1016/j.patcog.2003.11.003},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Attributes weights,Clustering,Data mining,Optimization},
number = {5},
pages = {943--952},
title = {{An optimization algorithm for clustering using weighted dissimilarity measures}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320303004035},
volume = {37},
year = {2004}
}
@inproceedings{Jing2005,
abstract = {This paper presents a new method to solve the problem of clustering large and complex text data. The method is based on a new subspace clustering algorithm that automatically calculates the feature weights in the k-means clustering process. In clustering sparse text data the feature weights are used to discover clusters from subspaces of the document vector space and identify key words that represent the semantics of the clusters. We present a modification of the published algorithm to solve the sparsity problem that occurs in text clustering. Experimental results on real-world text data have shown that the new method outperformed the Standard KMeans and Bisection-KMeans algorithms, while still maintaining efficiency of the k-means clustering process.},
address = {Berlin, Heidelberg},
author = {Jing, Liping and Ng, Michael K and Xu, Jun and Huang, Joshua Zhexue},
booktitle = {Advances in Knowledge Discovery and Data Mining},
editor = {Ho, Tu Bao and Cheung, David and Liu, Huan},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/fwkm.pdf:pdf},
isbn = {978-3-540-31935-1},
pages = {802--812},
publisher = {Springer Berlin Heidelberg},
title = {{Subspace Clustering of Text Documents with Feature Weighting K-Means Algorithm}},
year = {2005}
}
@article{Gan2016,
abstract = {Entropy weighting used in some soft subspace clustering algorithms is sensitive to the scaling parameter. In this paper, we propose a novel soft subspace clustering algorithm by using log-transformed distances in the objective function. The proposed algorithm allows users to choose a value of the scaling parameter easily because the entropy weighting in the proposed algorithm is less sensitive to the scaling parameter. In addition, the proposed algorithm is less sensitive to noises because a point far away from its cluster center is given a small weight in the cluster center calculation. Experiments on both synthetic datasets and real datasets are used to demonstrate the performance of the proposed algorithm.},
author = {Gan, Guojun and Chen, Kun},
doi = {10.3934/bdia.2016.1.93},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/ewkm-improved.pdf:pdf},
issn = {2380-6966},
journal = {Big Data and Information Analytics},
keywords = {Data clustering,attribute weighting,k-means,subspace clustering},
month = {sep},
number = {1},
pages = {93--109},
title = {{A soft subspace clustering algorithm with log-transformed distances}},
url = {http://aimsciences.org/article/id/a9eabafd-6990-44e7-bbf4-48f53319490a http://www.aimsciences.org/journals/displayArticlesnew.jsp?paperID=11640},
volume = {1},
year = {2015}
}
@article{ROUSSEEUW198753,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate' number of clusters.},
author = {Rousseeuw, Peter J},
doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
issn = {0377-0427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Graphical display,classification,cluster analysis,clustering validity},
pages = {53--65},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
url = {http://www.sciencedirect.com/science/article/pii/0377042787901257},
volume = {20},
year = {1987}
}
@techreport{Ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by factor of more than 100 in terms of efficiency.},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jiirg and Xu, Xiaowei},
file = {:home/ejuzovitski/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ester et al. - 1996 - A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.pdf:pdf},
keywords = {Arbitrary Shape of Clus-ters,Clustering Algorithms,Efficiency on Large Spatial Databases,Handling Nlj4-275oise},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
url = {www.aaai.org},
year = {1996}
}
@article{manning2010introduction,
author = {Manning, Christopher and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
journal = {Natural Language Engineering},
number = {1},
pages = {100--103},
publisher = {Cambridge university press},
title = {{Introduction to information retrieval}},
volume = {16},
year = {2010}
}
@article{Xu2015,
abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
author = {Xu, Dongkuan and Tian, Yingjie},
doi = {10.1007/s40745-015-0040-1},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/comprehensive-review.pdf:pdf},
issn = {2198-5812},
journal = {Annals of Data Science},
month = {jun},
number = {2},
pages = {165--193},
title = {{A Comprehensive Survey of Clustering Algorithms}},
url = {https://doi.org/10.1007/s40745-015-0040-1},
volume = {2},
year = {2015}
}
@article{Huang1998,
abstract = {The k-means algorithm is well known for its efficiency in clustering large data sets. However, working only on numeric values prohibits it from being used to cluster real world data containing categorical values. In this paper we present two algorithms which extend the k-means algorithm to categorical domains and domains with mixed numeric and categorical values. The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function. With these extensions the k-modes algorithm enables the clustering of categorical data in a fashion similar to k-means. The k-prototypes algorithm, through the definition of a combined dissimilarity measure, further integrates the k-means and k-modes algorithms to allow for clustering objects described by mixed numeric and categorical attributes. We use the well known soybean disease and credit approval data sets to demonstrate the clustering performance of the two algorithms. Our experiments on two real world data sets with half a million objects each show that the two algorithms are efficient when clustering large data sets, which is critical to data mining applications.},
author = {Huang, Zhexue},
doi = {10.1023/A:1009769707641},
issn = {1573-756X},
journal = {Data Mining and Knowledge Discovery},
month = {sep},
number = {3},
pages = {283--304},
title = {{Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values}},
url = {https://doi.org/10.1023/A:1009769707641},
volume = {2},
year = {1998}
}
@article{DAVID2012416,
abstract = {Data clustering is a common technique for data analysis, which is used in many fields, including machine learning, data mining, customer segmentation, trend analysis, pattern recognition and image analysis. Although many clustering algorithms have been proposed, most of them deal with clustering of one data type (numerical or nominal) or with mix data type (numerical and nominal) and only few of them provide a generic method that clusters all types of data. It is required for most real-world applications data to handle both feature types and their mix. In this paper, we propose an automated technique, called SpectralCAT, for unsupervised clustering of high-dimensional data that contains numerical or nominal or mix of attributes. We suggest to automatically transform the high-dimensional input data into categorical values. This is done by discovering the optimal transformation according to the Calinski–Harabasz index for each feature and attribute in the dataset. Then, a method for spectral clustering via dimensionality reduction of the transformed data is applied. This is achieved by automatic non-linear transformations, which identify geometric patterns in the data, and find the connections among them while projecting them onto low-dimensional spaces. We compare our method to several clustering algorithms using 16 public datasets from different domains and types. The experiments demonstrate that our method outperforms in most cases these algorithms.},
author = {David, Gil and Averbuch, Amir},
doi = {https://doi.org/10.1016/j.patcog.2011.07.006},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/spectralcat.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Categorical data clustering,Diffusion Maps,Dimensionality reduction,Spectral clustering},
number = {1},
pages = {416--433},
title = {{SpectralCAT: Categorical spectral clustering of numerical and nominal data}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320311002822},
volume = {45},
year = {2012}
}
@article{Deng2016,
abstract = {Subspace clustering (SC) is a promising technology involving clusters that are identified based on their association with subspaces in high-dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been studied extensively and are well accepted by the scientific community, SSC algorithms are relatively new. However, as they are said to be more adaptable than their HSC counterparts, SSC algorithms have been attracting more attention in recent years. A comprehensive survey of existing SSC algorithms and recent developments in the field are presented in this paper. SSC algorithms have been systematically classified into three main categories: conventional SSC (CSSC), independent SSC (ISSC), and extended SSC (XSSC). The characteristics of these algorithms are highlighted and potential future developments in the area of SSC are discussed. Through a comprehensive review of SSC, this paper aims to provide readers with a clear profile of existing SSC methods and to foster the development of more effective clustering technologies and significant research in this area.},
author = {Deng, Zhaohong and Choi, Kup-Sze and Jiang, Yizhang and Wang, Jun and Wang, Shitong},
doi = {https://doi.org/10.1016/j.ins.2016.01.101},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/survey{\_}subspace.pdf:pdf},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {Entropy weighting,Fuzzy C-means/-means model,Fuzzy weighting,Mixture model,Soft subspace clustering},
pages = {84--106},
title = {{A survey on soft subspace clustering}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025516300640},
volume = {348},
year = {2016}
}
@article{Kriegler2012,
abstract = {Abstract Subspace clustering refers to the task of identifying clusters of similar objects or data records (vectors) where the similarity is defined with respect to a subset of the attributes (i.e., a subspace of the data space). The subspace is not necessarily (and actually is usually not) the same for different clusters within one clustering solution. In this article, the problems motivating subspace clustering are sketched, different definitions and usages of subspaces for clustering are described, and exemplary algorithmic solutions are discussed. Finally, we sketch current research directions. {\textcopyright} 2012 Wiley Periodicals, Inc. This article is categorized under: Technologies {\textgreater} Structure Discovery and Clustering},
author = {Kriegel, Hans-Peter and Kr{\"{o}}ger, Peer and Zimek, Arthur},
doi = {10.1002/widm.1057},
file = {:home/ejuzovitski/Downloads/widm.1057.pdf:pdf},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {4},
pages = {351--364},
title = {{Subspace clustering}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1057},
volume = {2},
year = {2012}
}
@article{Jain1999,
address = {New York, NY, USA},
author = {Jain, A K and Murty, M N and Flynn, P J},
doi = {10.1145/331499.331504},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/oldreview.pdf:pdf},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
keywords = {cluster analysis,clustering applications,exploratory data analysis,incremental clustering,similarity indices,unsupervised learning},
month = {sep},
number = {3},
pages = {264--323},
publisher = {ACM},
title = {{Data Clustering: A Review}},
url = {http://doi.acm.org/10.1145/331499.331504},
volume = {31},
year = {1999}
}
@incollection{Paliwal2010,
abstract = {Publisher Summary This chapter provides an overview of an automatic speech recognition system and describes sources of speech variability that cause mismatch between training and testing. It also discusses some of the current techniques to achieve robust speech recognition. Automatic speech recognition is critical in natural human-centric interfaces for ambient intelligence. The performance of an automatic speech recognition system, however, degrades drastically when there is a mismatch between training and testing conditions. The aim of robust speech recognition is to overcome the mismatch problem so the result is a moderate and graceful degradation in recognition performance. The main factors that have made speech recognition possible are advances in digital signal processing (DSP) and stochastic modeling algorithms. Signal processing techniques are important for extracting reliable acoustic features from the speech signal, and stochastic modeling algorithms are useful for representing speech utterances in the form of efficient models, such as hidden Markov models (HMMs), which simplify the speech recognition task. Other factors responsible for the commercial success of speech recognition technology include the availability of fast processors (in the form of DSP chips) and high-density memories at relatively low cost. In the design of a practical robust speech recognition system for ambient intelligence, computational complexity is a very important factor. Thus, it is worthwhile to revise robust speech recognition methods in order to achieve simplified procedures, albeit with some performance losses. Balancing performance and computational cost for robust speech recognition for ambient intelligence will be a design art.},
address = {Oxford},
author = {Paliwal, Kuldip K and Yao, Kaisheng},
booktitle = {Human-Centric Interfaces for Ambient Intelligence},
doi = {https://doi.org/10.1016/B978-0-12-374708-2.00006-1},
editor = {Aghajan, Hamid and Delgado, Ram{\'{o}}n L{\'{o}}pez-C{\'{o}}zar and Augusto, Juan Carlos},
isbn = {978-0-12-374708-2},
pages = {135--162},
publisher = {Academic Press},
title = {{Chapter 6 - Robust Speech Recognition Under Noisy Ambient Conditions}},
url = {http://www.sciencedirect.com/science/article/pii/B9780123747082000061},
year = {2010}
}
@techreport{Arthur2006,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is {\$}O(\backslashlog k){\$}-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
institution = {Stanford InfoLab},
keywords = {clustering,k-means,seeding},
month = {jun},
number = {2006-13},
publisher = {Stanford},
title = {{k-means++: The Advantages of Careful Seeding}},
type = {Technical Report},
url = {8},
year = {2006}
}
@article{Sugar2003,
abstract = {One of the most difficult problems in cluster analysis is identifying the number of groups in a dataset. Most previously suggested approaches to this problem are either somewhat ad hoc or require parametric assumptions and complicated calculations. In this article we develop a simple, yet powerful nonparametric method for choosing the number of clusters based on distortion, a quantity that measures the average distance, per dimension, between each observation and its closest cluster center. Our technique is computationally efficient and straightforward to implement. We demonstrate empirically its effectiveness, not only for choosing the number of clusters, but also for identifying underlying structure, on a wide range of simulated and real world datasets. In addition, we give a rigorous theoretical justification for the method based on information-theoretic ideas. Specifically, results from the subfield of electrical engineering known as rate distortion theory allow us to describe the behavior of the distortion in both the presence and absence of clustering. Finally, we note that these ideas potentially can be extended to a wide range of other statistical model selection problems.},
author = {Sugar, Catherine A and James, Gareth M},
doi = {10.1198/016214503000000666},
journal = {Journal of the American Statistical Association},
number = {463},
pages = {750--763},
publisher = {Taylor {\&} Francis},
title = {{Finding the Number of Clusters in a Dataset}},
url = {https://doi.org/10.1198/016214503000000666},
volume = {98},
year = {2003}
}
@article{Novikov2019,
author = {Novikov, Andrei},
doi = {10.21105/joss.01230},
journal = {Journal of Open Source Software},
month = {apr},
number = {36},
pages = {1230},
publisher = {The Open Journal},
title = {{PyClustering: Data Mining Library}},
url = {https://doi.org/10.21105/joss.01230},
volume = {4},
year = {2019}
}
@inproceedings{Gan2006,
abstract = {In fuzzy clustering algorithms each object has a fuzzy membership associated with each cluster indicating the degree of association of the object to the cluster. Here we present a fuzzy subspace clustering algorithm, FSC, in which each dimension has a weight associated with each cluster indicating the degree of importance of the dimension to the cluster. Using fuzzy techniques for subspace clustering, our algorithm avoids the difficulty of choosing appropriate cluster dimensions for each cluster during the iterations. Our analysis and simulations strongly show that FSC is very efficient and the clustering results produced by FSC are very high in accuracy.},
address = {Berlin, Heidelberg},
author = {Gan, Guojun and Wu, Jianhong and Yang, Zijiang},
booktitle = {Advanced Data Mining and Applications},
editor = {Li, Xue and Za{\"{i}}ane, Osmar R and Li, Zhanhuai},
file = {:home/ejuzovitski/Documents/master{\_}thesis/docs/fsc-main.pdf:pdf},
isbn = {978-3-540-37026-0},
pages = {271--278},
publisher = {Springer Berlin Heidelberg},
title = {{A Fuzzy Subspace Algorithm for Clustering High Dimensional Data}},
year = {2006}
}
